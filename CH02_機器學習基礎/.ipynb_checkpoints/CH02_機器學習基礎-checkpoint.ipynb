{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二章機器學習基礎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機器學習起源於上世紀50年代，1959 年在 IBM 工作的 Arthur Samuel 設計了一個下棋程式，這個程式具有學習的能力，它可以在不斷的對弈中提高自己。從而提出了「機器學習」這個 概念，它是一個結合了多個學科如機率論，優化理論，統計等，最終在電腦上實現自我獲取新知識，學習改善自己的這樣一個研究領域。機器學習是人工智能的一個子集，目前已經發展出許多有用的方法，某些支持向量機(SVM)，回歸(Regression)，決策樹，隨機森林，強化方法，集成學習，深度學習等等，一定程度上可以幫助人們完成一些數據預測，自動化，自動決策，最優化 等初步替代腦力的任務。本章我們主要介紹下機器學習的基本概念：監督學習、分類算法、邏輯回歸、代價函數、損失函數、LDA、PCA、決策樹、支持向量機、EM算法、聚類和降維以及模型評估有一些方法、指標等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1機器學習本質"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機器學習（Machine Learning，ML），顧名思義，讓機器去學習。這裡，機器指的是計算機，是算法運行的物理載體，你也可以把各種算法本身當做一個有輸入和輸出的機器。那麼 對於一個任務及其表現的變量方法，設計一種算法，讓算法能夠提取中數據所蘊含的規律，則叫機器學習。如果輸入機器的數據是帶有標籤的 ，就在於有有監督學習。如果數據是無標籤的，就是無監督學習。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 什麼是神經網絡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神經網絡就是按照一定規則將多個神經元連接起來的網絡。不同的神經網絡，具有不同的連接規則。例如全連接（Full Connected，FC）<br>\n",
    "神經網絡的規則包括：\n",
    "\n",
    "- 有三個層：輸入層，輸出層，隱藏層。\n",
    "- 同一層的神經元之間沒有連接。\n",
    "- 完全連接的含義：第 N 層的每個神經元和第 N-1 層的所有神經元相連，第 N-1 層神經元的輸出就是第 N 層神經元的輸入。\n",
    "- 每個連接都有一個權值。\n",
    "\n",
    "**神經網絡架構**\n",
    "圖 2-1 是一個神經網絡系統，它由很多層組成。對輸入信息的傳遞和加工處理。\n",
    "![圖2-2神經網絡系統](img/ch2/2.5.1.png)\n",
    "\n",
    "圖 2-1 神經網絡系統"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 各種常見算法圖示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用機器學習中，我們經常會遇見各種算法，圖 2-2 是各種常見算法的圖示。\n",
    "\n",
    "|線性回歸(Linear Regression)|K-平均算法 (k-means clustering)|正規化 (Regularization)|\n",
    "| :----------------------:| :----------------------:| :----------------------:|\n",
    "|![](./img/ch2/2.1/1.jpg)|![](./img/ch2/2.1/2.jpg)|![](./img/ch2/2.1/3.jpg)|\n",
    "\n",
    "|決策樹 (Decision Tree)|貝氏推論 (Bayesian inference)|Kernel-Based Learning Algorithm|\n",
    "| :----------------------:| :----------------------:| :----------------------:|\n",
    "| ![](./img/ch2/2.2.4.png)| ![](./img/ch2/2.1/5.jpg)| ![](./img/ch2/2.1/6.jpg)|\n",
    "\n",
    "|集群分析 (Cluster Analysis)|關聯規則學習 (Association Rule Learning)|神經網絡 (Neural Network)|\n",
    "| :----------------------:| :----------------------:| :-----------------------:|\n",
    "| ![](./img/ch2/2.1/7.jpg)| ![](./img/ch2/2.2.8.png)| ![](./img/ch2/2.2.09.png)|\n",
    "\n",
    "|深度學習 (Deep Learning)|主成分分析 (Principal Component Analysis)|集成學習 (Ensemble Learning)|\n",
    "| :-----------------------:| :-----------------------:| :-----------------------:|\n",
    "| ![](./img/ch2/2.2.10.png)| ![](./img/ch2/2.2.11.png)| ![](./img/ch2/2.2.12.png)|\n",
    "\n",
    "圖 2-2 各種常見算法圖示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Computational Graph 的導數計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational Graph 的導數計算是反向傳播，利用 Chain Rule 和隱函式 (Implicit Equation) 求導。\n",
    "\n",
    "假設 $z=f(u, v)$ 在點 $(u, v)$ 處偏導連續，$(u, v)$ 是關於 $t$ 的函數，在 $t$ 點可導，求 $z$ 在 $t$ 點的導數。\n",
    "\n",
    "根據Chain Rule有：\n",
    "$$\n",
    "\\frac {dz}{dt}=\\frac {\\partial z}{\\partial u}\\frac {du}{dt}+\\frac{\\partial z}{\\partial v}\\frac {dv}{dt}\n",
    "$$\n",
    "\n",
    "Example：\n",
    "$$\n",
    "f(x)= x^{2}\n",
    "$$\n",
    "$$\n",
    "g(x)=2x+1\n",
    "$$\n",
    "\n",
    "則：\n",
    "$$\n",
    "{f(g(x))}'=2(g(x))g'(x)=2(2x+1)2=8x+4\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 局部最優解 與 全局最優解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "笑談局部最優和最大化\n",
    "\n",
    ">柏拉圖有一天問老師蘇格拉底什麼是愛情？蘇格拉底叫他到麥田走一次，摘一顆最大的麥穗回來，不許回頭，只可摘一次。柏拉圖空著手出來了，他的理由是，看見不錯的，卻不知道是不是最好的，一次次僥倖，走到盡頭時，才發現還不如前面的，於是放棄。蘇格拉底告訴他：“這就是愛情。”這故事讓我們明白了一個道理，因為生命的一些不確定性，所以可能最優解是很難尋找到的，或者說根本就不存在，我們應該設置一些限定條件，然後在這個範圍內尋找最優解，也就是局部最優解-有所斬獲總比空手而歸強，哪怕這種斬獲只是一次有趣的經歷。\n",
    ">柏拉圖有一天又問什麼是婚姻？蘇格拉底叫他到樹林走一次，選一棵最好的樹做聖誕樹，也是不許回頭，只許選一次。看起來直挺，翠綠，卻有點稀疏的杉樹回來，他的理由是，有了上回的教訓，好不容易看見一棵看似不錯的，又發現時間，體力已經快不夠用了，也不管是不是最好的，就拿回來了。蘇格拉底告訴他：“這就是婚姻。”\n",
    "\n",
    "最佳問題解答一般有局部最優解和最大化最優解：\n",
    "- 局部最優解：在函數值空間的一個有限區域內尋找代替解答(最接近)。<br>\n",
    "- 最優解：是在函數值空間整個區域尋找問題的最佳解答。<br>\n",
    "- 函數局部最小點是它的函數值小於或等於附近點的點，但是有可能大於較遠距離的點。<br>\n",
    "- 全局最小點是那種它的函數值小於或等於所有的可行點。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 大數據與深度學習之間的關係"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先來看大數據，機器學習及數據挖掘三者簡單的定義：\n",
    "\n",
    "**大數據**：通常被定義為「超過常用軟件工具捕獲，管理和處理能力」的數據集。<br>\n",
    "**機器學習**：關心問題是如何整合電腦程序使用經驗自動改進。<br>\n",
    "**數據挖掘**：是從數據中提取特定模式算法的應用，在數據挖掘中，重點在於算法的應用，而不是算法本身。<br>\n",
    "\n",
    "三者之間的關係如下：\n",
    "數據挖掘是一個過程，在此過程中機器學習算法被用來提取數據集中的潛在的潛在模式的工具。\n",
    "\n",
    "大數據與深度學習關係總結如下：\n",
    "- 深度學習是一種模擬大腦的行為。可以從所學習對象的機制以及行為等等很多相關聯的方面進行學習，模仿類型行為以及思維。<br>\n",
    "- 深度學習對於大數據技術開發的每一個階段幫助，無論是數據的分析還是挖掘還是建模，只有深度學習，這些工作才會有可能一一得到實現。<br>\n",
    "- 深度學習轉變了解決問題的思維。很多時候發現問題到解決問題，走一步算一步不再是主要的解決問題的方式。在深度學習的基礎上，要求我們從開始到最後都要依據一個目標，為了需要優化的那個最終目標去進行處理以及將數據放入到數據應用平台上去，這就是 End-to-End。<br>\n",
    "- 在大數據方面的深度學習都是從基礎的角度出發的，深度學習需要一個框架或者一個系統。總而言之，將大數據通過深度分析變為現實，這就是深度學習和大數據的最直接關係。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 機器學習學習方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根據數據類型的不同，每個問題的建模都會有不同的方式來依據不同的學習方式和輸入數據，機器學習主要分為以下學習方式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 監督式學習 (Supervised Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "數據：監督學習是使用已知正確答案的示例來訓練網絡。<br>\n",
    "定義：已知數據和其對應的標籤，訓練一個預測模型，將輸入數據映射到標籤的過程。<br>\n",
    "常見應用場景：監督式學習的常見應用場景如 分類問題 (Classification) 和 回歸問題 (Regression)。<br>\n",
    "\n",
    "常見算法：\n",
    "- Support Vector Machine (SVM)<br>\n",
    "- Naive Bayes<br>\n",
    "- Logistic Regression<br>\n",
    "- K-Nearest Neighborhood (KNN)<br>\n",
    "- Decision Tree<br>\n",
    "- Random Forest<br>\n",
    "- AdaBoost<br>\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "\n",
    "深度學習(Deep Learning) 大多也是以監督學習的方式呈現。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 非監督式學習 (Unsupervised Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "數據：在非監督式學習中，數據並不一定有標示<br>\n",
    "定義：適用於有數據集但無標籤的情況。此學習模型是為了替換找出數據的一些內在結構。<br>\n",
    "常見應用場景：通用的應用場景包括 Association Rule Learning 的學習以及聚類分析 (Cluster Analysis)等。<br>\n",
    "\n",
    "常見算法：\n",
    "- Apriori<br>\n",
    "- K-Means<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 半監督式學習 (Semi-supervised Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "數據：在此學習方式下，輸入數據部分被標記、部分沒有，這種學習模型可以進行預測。<br>\n",
    "常見應用場景：應用場景包括分類和回歸，算法包括一些針對常用 Supervised Learning 的延伸，通過對已標記數據建模，接此在此基礎上對未標記數據進行預測。<br>\n",
    "\n",
    "常用算法：\n",
    "- 論證推理算法（圖論）<br>\n",
    "- Laplace SVM<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 弱監督式學習 (Weakly Supervised Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "數據：弱監督學習可以看做是有多個標記的數據集合，e.g. 次集合可以是空集，個別元素，或包含多種情況<br>\n",
    "\n",
    "沒有標記、有一個標記或有多個標記的多數據集標籤基本上是不可靠的，這裡的不可靠可以是標記不正確、多種標記、標記不充分，局部標記等。<br>\n",
    "已知數據和其相對應的 弱標籤，訓練一個算法，將 input data 對應到一組 更強標籤 的過程。<br>\n",
    "標籤的強弱指的是標籤蘊含的信息量的多少，而相對於分割的標籤來說，分類的標籤就是弱標籤。<br>\n",
    "\n",
    "算法：重新計算，得到一張包含氣球的圖片，需要重新獲得氣球在圖片中的位置及氣球和背景的分割線，這就是已知的弱監督式學習強問題。\n",
    "\n",
    "企業數據應用的場景：雖然人們最常見的可能是監督式學習和非監督式學習的模型，但在圖像識別等領域，由於存在大量的非標識數據和少量可標識數據，因此被應用於此"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 監督式學習步驟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "監督式學習是使用已知正確答案的示例來訓練網絡，每組訓練數據有一個明確的標識或結果。想像一下，我們可以訓練一個網絡，讓其從照片中（其中包含氣球的照片）識別出氣球的照片。以下就是我們在這個假設場景中所要採取的步驟。\n",
    "\n",
    "**步驟1：Data Set 的創建和分類**\n",
    "首先，將包含氣球的照片進行標註 1，其餘標註 0。然後將所有照片分為訓練集和驗證集。目標就是在深度網絡中找一個函數，這個函數輸入是任意一張照片，當照片中包含氣球時，輸出 1，否則輸出 0。\n",
    "\n",
    "**步驟2：數據增強**\n",
    "當原始數據蒐集和標註完畢，一般蒐集的數據並不一定包含目標在各種擾動下的信息。數據的好壞對於機器學習模型的預測能力至關重要，因此一般會進行數據增加雜訊 (noise)。數據增強一般包括，圖像旋轉，平移，顏色變換，裁剪，仿射變換等。\n",
    "\n",
    "**步驟3：Feature Engineering**\n",
    "常見的特徵方式有 Scale-Invariant Feature Transform (SIFT)、Oriented Gradient (HOG)等。<br>\n",
    "由於手工特徵是啟發式的，其算法設計背後的出發點不同，將這些特徵組合在一起的時候有可能會產生衝突，如何將組合特徵的性能發揮出來，使原始數據在特徵空間中表現良好，然而在深度學習方法大獲成功之後，人們很大一部分不再關注特徵工程本身。<br>\n",
    "最常用到的 CNNs 本身就是一種特徵提取和選擇的引擎。(Normalization 實際上就是深度學習背景下的特徵工程)\n",
    "\n",
    "**步驟4：建立預測模型和損失**\n",
    "將原始數據映射到特徵空間之後，也就意味著我們得到了比較合理的輸入。下一步就是適當的預測模型得到對應的輸入的輸出。<br>\n",
    "而如何保證模型的輸出和輸入標籤的一致性，就需要優化模型預測和標籤之間的 Loss Function，常見的 Loss Function 有交叉熵 (Cross Entropy)、均方差 (Mean Squared Error，MSE)等。<br>\n",
    "通過優化方法不斷迭代，使模型從最初的初始化狀態一步步變化為有預測能力的模型的過程，實際上就是學習的過程。\n",
    "\n",
    "**步驟5：訓練**\n",
    "選擇合適的 model 和 hyperparametor 進行初始化，其中 hyperparametor 設置 SVM 中的 kernel function、誤差項懲罰權重等。<br>\n",
    "當模型初始化參數設置好後，將製作好的特徵數據輸入到模型，通過合適的優化方法不斷縮小輸出與標籤之間的差異，當交替過程到了終止條件，就可以得到訓練好的模型。<br>\n",
    "優化方法最常見的就是 Gradient Descent 及其變形。使用 Gradient Descent 的替代是優化目標函數針對模型要可微分的。\n",
    "\n",
    "**步驟6：驗證和模型選擇**\n",
    "訓練完訓練集圖片後，需要進行模型測試。利用驗證集來驗證模型是否可以準確地挑選出含有氣球內部的照片。\n",
    "在此過程中，通常會通過調整和模型相關的各種事物 (hyperparametor)來重複步驟 2 和 3，其中裡面有多少個節點、有多少層、使用怎樣的激活函數和損失函數，如何在反向傳播階段積極有效地訓練權值等等。\n",
    "\n",
    "**步驟7：測試及應用**\n",
    "當有了一個準確的模型，就可以通過模型部署到您的應用程序中。您可以將預測功能發佈為 API，並且從軟體中調用該 API 從而進行推理並提供相應的結果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 分類算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分類算法 (Classification) 和回歸算法 (Regression) 是對真實世界不同建模的方法。<br>\n",
    "Classification Model 是認為模型的輸出為離散。E.g. 大自然的生物被劃分為不同的種類，是離散的\n",
    "Regression Model 的輸出是連續的。E.g. 人的身高變化過程是一個連續過程\n",
    "\n",
    "因此，在實際建模過程時，採用分類模型還是回歸模型，取決於你對真實世界的分析和理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 常用分類算法的優缺點？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下來我們介紹常用分類算法的優缺點，如表2-1所示。\n",
    "\n",
    "表2-1常用分類算法的優缺點\n",
    "\n",
    "|算法|優點|缺點|\n",
    "|:---:|:---:|:---:|\n",
    "|Naive Bayes | 1)所需的估計參數少，對於缺失數據不敏感<br/> 2)擁有的數學基礎，以及穩定的分類效率| 1)需要假設屬性之間相互獨立(喜歡吃番茄、雞蛋，卻不喜歡吃番茄炒蛋)<br/> 2)需要知道先覺機率<br/> 3)分類方法存在錯誤率|\n",
    "|Decision Tree| 1)不需要任何領域知識或參數假設<br/> 2)適合高維數據<br/> 3)簡單易理解<br/> 4)短時間內處理大量數據<br/> 5)能夠同時處理數據類型和常規性屬性| 1)關於各個類別樣本數量的數據，訊息增益偏向於那些具有更多樣本數據的特徵<br/> 2)Over Fitting<br/> 3)忽略屬性之間的相關性<br/> 4)Not support Linear Learning|\n",
    "| SVM| 1)可以解決小樣本下的機器學習的問題<br/> 2)提高 generalization 性能<br/> 3)可以解決高維、非線性問題、超高維文本分類仍受歡迎<br/> 4)避免神經網絡結構選擇和local minimum 的問題| 1)對缺失數據敏感<br/> 2)Ram 消耗大<br/> 3)運行和參數調整比較麻煩|\n",
    "| K-Nearest Neighbors (KNN)| 1)概念簡單，理論成熟，可以用來做分類或回歸<br/> 2)可用於非線性分類<br/> 3)訓練時間複雜度為O(n)<br/> 4)準確度高，對數據沒有假設，對outlier不敏感 | 1)計算量太大<br/> 2)對於樣本分類不均衡的問題，會產生誤判 (imbalance)<br/> 3)需要大量的Ram<br/> 4)輸出的可解釋性不強|\n",
    "|Logistic Regression| 1)速度快<br/> 2)簡單易懂，直接看到各個特徵的權重<br/> 3)能容易地更新模型吸收新的數據<br/> 4 )有一個機率框架可動態調整分類閥值|特徵處理複雜需要 Normalize 和對準的特徵工程\n",
    "|Neural Network| 1)分類準確率高<br/> 2)並行處理能力強<br/> 3)分佈式存儲和學習能力強<br/> 4)Robustness| 1)需要大量參數(網絡拓撲，閥值，閾值)<br/> 2)結果難以解釋<br/> 3)訓練時間過長|\n",
    "|Adaboosting| 1)adaboost是一種有很精細的分類器<br/> 2)可以使用各種方法集成子分類器，Adaboost算法提供的是框架<br/> 3)簡單分類<br/> 4)簡單，不用做特徵篩選<br/> 5)不用擔心過度擬合|對異常比較敏感|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
