{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二章機器學習基礎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機器學習起源於上世紀50年代，1959 年在 IBM 工作的 Arthur Samuel 設計了一個下棋程式，這個程式具有學習的能力，它可以在不斷的對弈中提高自己。從而提出了「機器學習」這個 概念，它是一個結合了多個學科如機率論，優化理論，統計等，最終在電腦上實現自我獲取新知識，學習改善自己的這樣一個研究領域。機器學習是人工智能的一個子集，目前已經發展出許多有用的方法，某些支持向量機(SVM)，回歸(Regression)，決策樹，隨機森林，強化方法，集成學習，深度學習等等，一定程度上可以幫助人們完成一些數據預測，自動化，自動決策，最優化 等初步替代腦力的任務。本章我們主要介紹下機器學習的基本概念：監督學習、分類算法、邏輯回歸、代價函數、損失函數、LDA、PCA、決策樹、支持向量機、EM算法、聚類和降維以及模型評估有一些方法、指標等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1機器學習本質"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機器學習（Machine Learning，ML），顧名思義，讓機器去學習。這裡，機器指的是計算機，是算法運行的物理載體，你也可以把各種算法本身當做一個有輸入和輸出的機器。那麼 對於一個任務及其表現的變量方法，設計一種算法，讓算法能夠提取中數據所蘊含的規律，則叫機器學習。如果輸入機器的數據是帶有標籤的 ，就在於有有監督學習。如果數據是無標籤的，就是無監督學習。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 什麼是神經網絡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神經網絡就是按照一定規則將多個神經元連接起來的網絡。不同的神經網絡，具有不同的連接規則。例如全連接（Full Connected，FC）<br>\n",
    "神經網絡的規則包括：\n",
    "\n",
    "- 有三個層：輸入層，輸出層，隱藏層。\n",
    "- 同一層的神經元之間沒有連接。\n",
    "- 完全連接的含義：第 N 層的每個神經元和第 N-1 層的所有神經元相連，第 N-1 層神經元的輸出就是第 N 層神經元的輸入。\n",
    "- 每個連接都有一個權值。\n",
    "\n",
    "**神經網絡架構**\n",
    "圖 2-1 是一個神經網絡系統，它由很多層組成。對輸入信息的傳遞和加工處理。\n",
    "![圖2-2神經網絡系統](img/ch2/2.5.1.png)\n",
    "\n",
    "圖 2-1 神經網絡系統"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 各種常見算法圖示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用機器學習中，我們經常會遇見各種算法，圖 2-2 是各種常見算法的圖示。\n",
    "\n",
    "|線性回歸(Linear Regression)|K-平均算法 (k-means clustering)|正規化 (Regularization)|\n",
    "| :----------------------:| :----------------------:| :----------------------:|\n",
    "|![](./img/ch2/2.1/1.jpg)|![](./img/ch2/2.1/2.jpg)|![](./img/ch2/2.1/3.jpg)|\n",
    "\n",
    "|決策樹 (Decision Tree)|貝氏推論 (Bayesian inference)|Kernel-Based Learning Algorithm|\n",
    "| :----------------------:| :----------------------:| :----------------------:|\n",
    "| ![](./img/ch2/2.2.4.png)| ![](./img/ch2/2.1/5.jpg)| ![](./img/ch2/2.1/6.jpg)|\n",
    "\n",
    "|集群分析 (Cluster Analysis)|關聯規則學習 (Association Rule Learning)|神經網絡 (Neural Network)|\n",
    "| :----------------------:| :----------------------:| :-----------------------:|\n",
    "| ![](./img/ch2/2.1/7.jpg)| ![](./img/ch2/2.2.8.png)| ![](./img/ch2/2.2.09.png)|\n",
    "\n",
    "|深度學習 (Deep Learning)|主成分分析 (Principal Component Analysis)|集成學習 (Ensemble Learning)|\n",
    "| :-----------------------:| :-----------------------:| :-----------------------:|\n",
    "| ![](./img/ch2/2.2.10.png)| ![](./img/ch2/2.2.11.png)| ![](./img/ch2/2.2.12.png)|\n",
    "\n",
    "圖 2-2 各種常見算法圖示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Computational Graph 的導數計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational Graph 的導數計算是反向傳播，利用 Chain Rule 和隱函式 (Implicit Equation) 求導。\n",
    "\n",
    "假設 $z=f(u, v)$ 在點 $(u, v)$ 處偏導連續，$(u, v)$ 是關於 $t$ 的函數，在 $t$ 點可導，求 $z$ 在 $t$ 點的導數。\n",
    "\n",
    "根據Chain Rule有：\n",
    "$$\n",
    "\\frac {dz}{dt}=\\frac {\\partial z}{\\partial u}\\frac {du}{dt}+\\frac{\\partial z}{\\partial v}\\frac {dv}{dt}\n",
    "$$\n",
    "\n",
    "Example：\n",
    "$$\n",
    "f(x)= x^{2}\n",
    "$$\n",
    "$$\n",
    "g(x)=2x+1\n",
    "$$\n",
    "\n",
    "則：\n",
    "$$\n",
    "{f(g(x))}'=2(g(x))g'(x)=2(2x+1)2=8x+4\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 局部最優解 與 全局最優解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "笑談局部最優和最大化\n",
    "\n",
    ">柏拉圖有一天問老師蘇格拉底什麼是愛情？蘇格拉底叫他到麥田走一次，摘一顆最大的麥穗回來，不許回頭，只可摘一次。柏拉圖空著手出來了，他的理由是，看見不錯的，卻不知道是不是最好的，一次次僥倖，走到盡頭時，才發現還不如前面的，於是放棄。蘇格拉底告訴他：“這就是愛情。”這故事讓我們明白了一個道理，因為生命的一些不確定性，所以可能最優解是很難尋找到的，或者說根本就不存在，我們應該設置一些限定條件，然後在這個範圍內尋找最優解，也就是局部最優解-有所斬獲總比空手而歸強，哪怕這種斬獲只是一次有趣的經歷。\n",
    ">柏拉圖有一天又問什麼是婚姻？蘇格拉底叫他到樹林走一次，選一棵最好的樹做聖誕樹，也是不許回頭，只許選一次。看起來直挺，翠綠，卻有點稀疏的杉樹回來，他的理由是，有了上回的教訓，好不容易看見一棵看似不錯的，又發現時間，體力已經快不夠用了，也不管是不是最好的，就拿回來了。蘇格拉底告訴他：“這就是婚姻。”\n",
    "\n",
    "最佳問題解答一般有局部最優解和最大化最優解：\n",
    "- 局部最優解：在函數值空間的一個有限區域內尋找代替解答(最接近)。<br>\n",
    "- 最優解：是在函數值空間整個區域尋找問題的最佳解答。<br>\n",
    "- 函數局部最小點是它的函數值小於或等於附近點的點，但是有可能大於較遠距離的點。<br>\n",
    "- 全局最小點是那種它的函數值小於或等於所有的可行點。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 大數據與深度學習之間的關係"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先來看大數據，機器學習及數據挖掘三者簡單的定義：\n",
    "\n",
    "**大數據**：通常被定義為「超過常用軟件工具捕獲，管理和處理能力」的數據集。<br>\n",
    "**機器學習**：關心問題是如何整合電腦程序使用經驗自動改進。<br>\n",
    "**數據挖掘**：是從數據中提取特定模式算法的應用，在數據挖掘中，重點在於算法的應用，而不是算法本身。<br>\n",
    "\n",
    "三者之間的關係如下：\n",
    "數據挖掘是一個過程，在此過程中機器學習算法被用來提取數據集中的潛在的潛在模式的工具。\n",
    "\n",
    "大數據與深度學習關係總結如下：\n",
    "- 深度學習是一種模擬大腦的行為。可以從所學習對象的機制以及行為等等很多相關聯的方面進行學習，模仿類型行為以及思維。<br>\n",
    "- 深度學習對於大數據技術開發的每一個階段幫助，無論是數據的分析還是挖掘還是建模，只有深度學習，這些工作才會有可能一一得到實現。<br>\n",
    "- 深度學習轉變了解決問題的思維。很多時候發現問題到解決問題，走一步算一步不再是主要的解決問題的方式。在深度學習的基礎上，要求我們從開始到最後都要依據一個目標，為了需要優化的那個最終目標去進行處理以及將數據放入到數據應用平台上去，這就是 End-to-End。<br>\n",
    "- 在大數據方面的深度學習都是從基礎的角度出發的，深度學習需要一個框架或者一個系統。總而言之，將大數據通過深度分析變為現實，這就是深度學習和大數據的最直接關係。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 機器學習學習方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根據數據類型的不同，每個問題的建模都會有不同的方式來依據不同的學習方式和輸入數據，機器學習主要分為以下學習方式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 監督式學習 (Supervised Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "數據：監督學習是使用已知正確答案的示例來訓練網絡。<br>\n",
    "定義：已知數據和其對應的標籤，訓練一個預測模型，將輸入數據映射到標籤的過程。<br>\n",
    "常見應用場景：監督式學習的常見應用場景如 分類問題 (Classification) 和 回歸問題 (Regression)。<br>\n",
    "\n",
    "常見算法：\n",
    "- Support Vector Machine (SVM)<br>\n",
    "- Naive Bayes<br>\n",
    "- Logistic Regression<br>\n",
    "- K-Nearest Neighborhood (KNN)<br>\n",
    "- Decision Tree<br>\n",
    "- Random Forest<br>\n",
    "- AdaBoost<br>\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "\n",
    "深度學習(Deep Learning) 大多也是以監督學習的方式呈現。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 非監督式學習 (Unsupervised Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "數據：在非監督式學習中，數據並不一定有標示<br>\n",
    "定義：適用於有數據集但無標籤的情況。此學習模型是為了替換找出數據的一些內在結構。<br>\n",
    "常見應用場景：通用的應用場景包括 Association Rule Learning 的學習以及聚類分析 (Cluster Analysis)等。<br>\n",
    "\n",
    "常見算法：\n",
    "- Apriori<br>\n",
    "- K-Means<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 半監督式學習 (Semi-supervised Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "數據：在此學習方式下，輸入數據部分被標記、部分沒有，這種學習模型可以進行預測。<br>\n",
    "常見應用場景：應用場景包括分類和回歸，算法包括一些針對常用 Supervised Learning 的延伸，通過對已標記數據建模，接此在此基礎上對未標記數據進行預測。<br>\n",
    "\n",
    "常用算法：\n",
    "- 論證推理算法（圖論）<br>\n",
    "- Laplace SVM<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 弱監督式學習 (Weakly Supervised Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "數據：弱監督學習可以看做是有多個標記的數據集合，e.g. 次集合可以是空集，個別元素，或包含多種情況<br>\n",
    "\n",
    "沒有標記、有一個標記或有多個標記的多數據集標籤基本上是不可靠的，這裡的不可靠可以是標記不正確、多種標記、標記不充分，局部標記等。<br>\n",
    "已知數據和其相對應的 弱標籤，訓練一個算法，將 input data 對應到一組 更強標籤 的過程。<br>\n",
    "標籤的強弱指的是標籤蘊含的信息量的多少，而相對於分割的標籤來說，分類的標籤就是弱標籤。<br>\n",
    "\n",
    "算法：重新計算，得到一張包含氣球的圖片，需要重新獲得氣球在圖片中的位置及氣球和背景的分割線，這就是已知的弱監督式學習強問題。\n",
    "\n",
    "企業數據應用的場景：雖然人們最常見的可能是監督式學習和非監督式學習的模型，但在圖像識別等領域，由於存在大量的非標識數據和少量可標識數據，因此被應用於此"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 監督式學習步驟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "監督式學習是使用已知正確答案的示例來訓練網絡，每組訓練數據有一個明確的標識或結果。想像一下，我們可以訓練一個網絡，讓其從照片中（其中包含氣球的照片）識別出氣球的照片。以下就是我們在這個假設場景中所要採取的步驟。\n",
    "\n",
    "**步驟1：Data Set 的創建和分類**\n",
    "首先，將包含氣球的照片進行標註 1，其餘標註 0。然後將所有照片分為訓練集和驗證集。目標就是在深度網絡中找一個函數，這個函數輸入是任意一張照片，當照片中包含氣球時，輸出 1，否則輸出 0。\n",
    "\n",
    "**步驟2：數據增強**\n",
    "當原始數據蒐集和標註完畢，一般蒐集的數據並不一定包含目標在各種擾動下的信息。數據的好壞對於機器學習模型的預測能力至關重要，因此一般會進行數據增加雜訊 (noise)。數據增強一般包括，圖像旋轉，平移，顏色變換，裁剪，仿射變換等。\n",
    "\n",
    "**步驟3：Feature Engineering**\n",
    "常見的特徵方式有 Scale-Invariant Feature Transform (SIFT)、Oriented Gradient (HOG)等。<br>\n",
    "由於手工特徵是啟發式的，其算法設計背後的出發點不同，將這些特徵組合在一起的時候有可能會產生衝突，如何將組合特徵的性能發揮出來，使原始數據在特徵空間中表現良好，然而在深度學習方法大獲成功之後，人們很大一部分不再關注特徵工程本身。<br>\n",
    "最常用到的 CNNs 本身就是一種特徵提取和選擇的引擎。(Normalization 實際上就是深度學習背景下的特徵工程)\n",
    "\n",
    "**步驟4：建立預測模型和損失**\n",
    "將原始數據映射到特徵空間之後，也就意味著我們得到了比較合理的輸入。下一步就是適當的預測模型得到對應的輸入的輸出。<br>\n",
    "而如何保證模型的輸出和輸入標籤的一致性，就需要優化模型預測和標籤之間的 Loss Function，常見的 Loss Function 有交叉熵 (Cross Entropy)、均方差 (Mean Squared Error，MSE)等。<br>\n",
    "通過優化方法不斷迭代，使模型從最初的初始化狀態一步步變化為有預測能力的模型的過程，實際上就是學習的過程。\n",
    "\n",
    "**步驟5：訓練**\n",
    "選擇合適的 model 和 hyperparametor 進行初始化，其中 hyperparametor 設置 SVM 中的 kernel function、誤差項懲罰權重等。<br>\n",
    "當模型初始化參數設置好後，將製作好的特徵數據輸入到模型，通過合適的優化方法不斷縮小輸出與標籤之間的差異，當交替過程到了終止條件，就可以得到訓練好的模型。<br>\n",
    "優化方法最常見的就是 Gradient Descent 及其變形。使用 Gradient Descent 的替代是優化目標函數針對模型要可微分的。\n",
    "\n",
    "**步驟6：驗證和模型選擇**\n",
    "訓練完訓練集圖片後，需要進行模型測試。利用驗證集來驗證模型是否可以準確地挑選出含有氣球內部的照片。\n",
    "在此過程中，通常會通過調整和模型相關的各種事物 (hyperparametor)來重複步驟 2 和 3，其中裡面有多少個節點、有多少層、使用怎樣的激活函數和損失函數，如何在反向傳播階段積極有效地訓練權值等等。\n",
    "\n",
    "**步驟7：測試及應用**\n",
    "當有了一個準確的模型，就可以通過模型部署到您的應用程序中。您可以將預測功能發佈為 API，並且從軟體中調用該 API 從而進行推理並提供相應的結果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 分類算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分類算法 (Classification) 和回歸算法 (Regression) 是對真實世界不同建模的方法。<br>\n",
    "Classification Model 是認為模型的輸出為離散。E.g. 大自然的生物被劃分為不同的種類，是離散的\n",
    "Regression Model 的輸出是連續的。E.g. 人的身高變化過程是一個連續過程\n",
    "\n",
    "因此，在實際建模過程時，採用分類模型還是回歸模型，取決於你對真實世界的分析和理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 常用分類算法的優缺點？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下來我們介紹常用分類算法的優缺點，如表2-1所示。\n",
    "\n",
    "表2-1常用分類算法的優缺點\n",
    "\n",
    "|算法|優點|缺點|\n",
    "|:---:|:---:|:---:|\n",
    "|Naive Bayes | 1)所需的估計參數少，對於缺失數據不敏感<br/> 2)擁有的數學基礎，以及穩定的分類效率| 1)需要假設屬性之間相互獨立(喜歡吃番茄、雞蛋，卻不喜歡吃番茄炒蛋)<br/> 2)需要知道先覺機率<br/> 3)分類方法存在錯誤率|\n",
    "|Decision Tree| 1)不需要任何領域知識或參數假設<br/> 2)適合高維數據<br/> 3)簡單易理解<br/> 4)短時間內處理大量數據<br/> 5)能夠同時處理數據類型和常規性屬性| 1)關於各個類別樣本數量的數據，訊息增益偏向於那些具有更多樣本數據的特徵<br/> 2)Over Fitting<br/> 3)忽略屬性之間的相關性<br/> 4)Not support Linear Learning|\n",
    "| SVM| 1)可以解決小樣本下的機器學習的問題<br/> 2)提高 generalization 性能<br/> 3)可以解決高維、非線性問題、超高維文本分類仍受歡迎<br/> 4)避免神經網絡結構選擇和local minimum 的問題| 1)對缺失數據敏感<br/> 2)Ram 消耗大<br/> 3)運行和參數調整比較麻煩|\n",
    "| K-Nearest Neighbors (KNN)| 1)概念簡單，理論成熟，可以用來做分類或回歸<br/> 2)可用於非線性分類<br/> 3)訓練時間複雜度為O(n)<br/> 4)準確度高，對數據沒有假設，對outlier不敏感 | 1)計算量太大<br/> 2)對於樣本分類不均衡的問題，會產生誤判 (imbalance)<br/> 3)需要大量的Ram<br/> 4)輸出的可解釋性不強|\n",
    "|Logistic Regression| 1)速度快<br/> 2)簡單易懂，直接看到各個特徵的權重<br/> 3)能容易地更新模型吸收新的數據<br/> 4 )有一個機率框架可動態調整分類閥值|特徵處理複雜需要 Normalize 和對準的特徵工程\n",
    "|Neural Network| 1)分類準確率高<br/> 2)並行處理能力強<br/> 3)分佈式存儲和學習能力強<br/> 4)Robustness| 1)需要大量參數(網絡拓撲，閥值，閾值)<br/> 2)結果難以解釋<br/> 3)訓練時間過長|\n",
    "|Adaboosting| 1)adaboost是一種有很精細的分類器<br/> 2)可以使用各種方法集成子分類器，Adaboost算法提供的是框架<br/> 3)簡單分類<br/> 4)簡單，不用做特徵篩選<br/> 5)不用擔心過度擬合|對異常比較敏感|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 分類算法的評估方法\n",
    "$()$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分類評估方法主要功能是用來評估分類算法的好壞，而評估一個分類器算法的好壞又包括許多項指標。<br>\n",
    "了解各種評估方法，在實際應用中選擇正確的評估方法是十分重要的。<br>\n",
    "\n",
    "- **常用術語**：這里首先介紹幾個常見模型評價的術語，現在假設我們的分類目標只有兩類，計為正例（positive）和負例（negative）分別是：<br>\n",
    "    1) True Positives(TP): 被正確地劃分為正例的個數，即實際為正例且被分類器劃分為正例的實例數<br>\n",
    "    2) False Positives(FP): 被錯誤地劃分為正例的個數，即實際為負例但被分類器劃分為正例的實例數<br>\n",
    "    3) False Negatives(FN):被錯誤地劃分為負例的個數，即實際為正例但被分類器劃分為負例的實例數<br>\n",
    "    4) True Negatives(TN): 被正確地劃分為負例的個數，即實際為負例且被分類器劃分為負例的實例數<br>\n",
    "\n",
    "    表2-2 4個術語的混淆矩陣\n",
    "\n",
    "    ![圖2-3 術語的混淆矩陣](./img/ch2/2.9/1.png)\n",
    "\n",
    "    表 2-2 是這四個術語的混淆矩陣，做以下說明：<br>\n",
    "    1) $P = TP + FN$ 表示實際為正例的樣本個數。<br>\n",
    "    2) $True$、$False$ 描述的是分類器是否判斷正確。<br>\n",
    "    3) $Positive$、$Negative$ 是分類器的分類結果，如果正例計為 1、負例計為 -1，即 $Positive=1$、$Negative=-1$。用 1 表示 $True$，-1 表示 $False$，那麼實際的類標等於 $\\frac{TF}{PN}$，$TF$ 為 $True$ 或 $False$，$PN$ 為 $Positive$ 或 $Negative$。<br>\n",
    "    4) Example:<br>\n",
    "     $True\\,Positives\\,(TP)$ 的實際類標等於 $\\frac{1}{1}=1$ 為正例<br>\n",
    "     $False\\,Positives\\,(FP)$ 的實際類標等於 $\\frac{-1}{1}=-1$ 為負例<br>\n",
    "     $False\\,Negatives\\,(FN)$ 的實際類標等於 $\\frac{-1}{-1}=1$ 為正例<br>\n",
    "     $True\\,Negatives\\,(TN)$ 的實際類標等於 $\\frac{1}{-1}=-1$ 為負例<br>\n",
    "\n",
    "\n",
    "- **評價指標**<br>\n",
    "  1) 正確率 (accuracy)：正確率是我們最常見的評價指標，$accuracy=\\frac{TP+TN}{P+N}$，正確率是被分對的樣本數在所有樣本數中的佔比，通常來說，正確率越高，分類器越好。<br>\n",
    "  2) 錯誤率 (error rate)：錯誤率則與正確率相反，描述被分類器錯分的比例，$error\\,rate=\\frac{FP+FN}{P+N}$，對某一個實例來說，分對與分錯是互斥 (independent) 事件，所以 $accuracy=1-error$。<br>\n",
    "  3) 靈敏度 (sensitivity)：$sensitivity=\\frac{TP}{P}$，表示的是所有正例中被分對的比例，衡量了分類器對正例的判斷能力<br>\n",
    "  4) 特異性 (specificity)：$specificity=\\frac{TN}{N}$，表示的是所有負例中被分對的比例，衡量了分類器對負例的判斷能力<br>\n",
    "  5) 精準度 (precision)：$precision=\\frac{TP}{TP+FP}$，精準度是精確性的衡量，表示被分為正例的示例中實際為正例的比例<br>\n",
    "  6) 召回率 (recall)：$recall=\\frac{TP}{TP+FN}=\\frac{TP}{P}=sensitivity$，召回率是覆蓋面的衡量，用以表示有多個正例被分為正例，可以看到召回率與靈敏度是一樣的<br>\n",
    "  7) 其他評價指標：<br>\n",
    "        計算速度：分類器訓練和預測所需要的時間\n",
    "        魯棒性 (Robustness)：處理缺失值和異常值的能力\n",
    "        可擴展性：處理大數據集的能力\n",
    "        可解釋性：分類器的預測標準的可理解性，像決策樹產生的規則就是容易瞭解，而神經網絡的一堆參數就不好理解，我們只好把它看成一個黑盒子\n",
    "  8) precision 和 recall 反映了分類器分類性能的兩個方面。如果綜合考慮 查準率 與 查全率，可以得到新的評價指標 F1-score，也稱為綜合分類率：$F1=\\frac{2 \\times precision \\times recall}{precision + recall}$。\n",
    "\n",
    "為了綜合多個類別的分類情況，評測系統整體性能，經常採用的還有微平均 $F1\\,(micro-averaging)$ 和 宏平均 $F1\\,(macro-averaging)$ 兩種指標<br>\n",
    "    (1) $F1\\,(macro-averaging)$ 與 $F1\\,(micro-averaging)$ 是以兩種不同的平均方式求的全局 $F1$ 指標<br>\n",
    "    (2) $F1\\,(macro-averaging)$ 的計算方法先對每個類別單獨計算 $F1$ 值，再取這些 $F1$ 值的算術平均值作為全局指標<br>\n",
    "    (3) $F1\\,(micro-averaging)$ 的計算方法是先累加計算各個類別的 $a$、$b$、$c$、$d$ 的值，再由這些值求出 $F1$ 值<br>\n",
    "    (4) 由兩種平均 $F1$ 的計算方式不難看出，$F1\\,(macro-averaging)$ 平等對待每一個類別，所以它的值主要受到稀有類別的影響；而$F1\\,(micro-averaging)$ 平等考慮文檔集中的每一個文檔，所以它的值受到常見類別的影響比較大。<br>\n",
    "\n",
    "- **ROC曲線 和 PR曲線**\n",
    "\n",
    "如圖 2-3，ROC 曲線是 (Receiver Operating Characteristic Curve，受試者工作特徵曲線) 的簡稱，是以靈敏度 (sensitivity)為縱坐標，以 1 - 特異性 (specificity)為橫坐標繪製的性能評價曲線。可以將不同模型對同一數據集的 ROC 曲線繪製在同一笛卡爾坐標系中<br>\n",
    "ROC曲線越靠近左上角，說明其對應模型越準確。也可以通過 ROC 曲線下面的面積 Area Under Curve (AUC) 來評斷模型，AUC越大，模型越準確。\n",
    "\n",
    "![](./img/ch2/2.7.3.png)\n",
    "\n",
    "圖 2-3 ROC曲線\n",
    "\n",
    "PR 曲線是 Precision Recall Curve 的簡稱，描述的是 precision 和 recall 之間的關係，以 recall 為橫坐標，precision 為縱坐標繪製的曲線。該曲線的所對應的面積 AUC 實際上是目標檢測中常用的評價指標平均精度 (Average Precision, AP)<br>\n",
    "AP越高，說明模型性能越好\n",
    "\n",
    "![](./img/ch2/pr_roc.png)\n",
    "\n",
    "圖 2-4 PR 曲線 v.s. ROC 曲線\n",
    "\n",
    "Note：<br>\n",
    "(1) ROC Curves 適合類別平均的情況，而 PR Curves 適合於類別不平均的情況<br>\n",
    "(2) ROC 曲線同時考慮了正例以及負例，因此適用於評估分類器整體的效能，而 PR 曲線則專注於正例，若是在類別平均且正例及負例的判斷都重要的情況下，ROC 曲線是不錯的選擇<br>\n",
    "(3) 由於 ROC 曲線的 X 軸使用到了 FPR，在類別不平均的情況下(負樣本較多)，使得 FPR 的增長會被稀釋，會導致 ROC 曲線呈現出過度樂觀的結果，因此在類別不平均的情況下，PR 曲線會是較好的選擇<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 正確率能否有效評斷分類算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 不同算法有不同特點，在不同數據集上有不同的表現效果，根據特定的任務選擇不同的算法。<br>\n",
    "    如何評價分類算法的好壞，要針對具體任務進行分析。<br>\n",
    "    對於決策樹，主要用正確率去評估，但是其他算法，只用正確率能很好的評估嗎？\n",
    "- 答案是否定的。\n",
    "- 正確率確實是一個很直觀很好的評價指標，但是有時候正確率高並不能完全代表一個算法就好。<br>\n",
    "    比如對某個地區進行地震預測，地震分類屬性分為 0：不發生地震、1 發生地震。我們都知道，不發生的概率是極大的，對於分類器而言，如果分類器不加思考，對每一個測試樣例的類別都劃分為 0，達到 99% 的正確率，但是，問題來了，如果真的發生地震時，這個分類器毫無察覺，那將帶來巨大的後果。<br>\n",
    "    很顯然，99% 正確率的分類器並不是我們想要的。出現這種現象的原因主要是數據分佈不均衡，類別為 1 的數據太少，錯分了類別 1 但達到了很高的正確率缺忽視了研究者本身最為關注的情況。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 什麼樣的分類器是最好的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 對某一個項目來說，某個具體的分類器不可能同時滿足或提高所有上面介紹的指標。<br>\n",
    "- 如果一個分類器能正確分對所有的實例，那麼各項指標都已經達到最優，但這樣的分類器往往不存在。<br>\n",
    "    比如之前說的地震預測，既然不能百分百預測地震的發生，但實際情況中能容忍一定程度的誤報。<br>\n",
    "    假設在 1000 次預測中，共有 5 次預測發生了地震，真實情況中有一次發生了地震，其他 4 次則為誤報。<br>\n",
    "    \n",
    "    正確率由原來的 $\\frac{999}{1000}=99.9\\%$ 下降為$\\frac{996}{1000}=99.6\\%$<br>\n",
    "    Recall 由 $\\frac{0}{1}=0\\%$ 上升為 $\\frac{1}{1}=100\\%$。<br>\n",
    "    \n",
    "    對此解釋為，雖然預測失誤了 4 次，但真的地震發生前，分類器能預測對，沒有錯過，這樣的分類器實際意義更為重大，正是我們想要的。在這種情況下，在一定正確率前提下，要求分類器的 Recall 盡量高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
