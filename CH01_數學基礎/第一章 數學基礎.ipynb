{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RuYmWSZ5YhQh"
   },
   "source": [
    "# 第一章數學基礎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RuYmWSZ5YhQh"
   },
   "source": [
    "深度學習通常又需要某些數學基礎？這些數學知識有相關性，但實際上按照這樣的知識範圍來學習，學習成本會很久，而且會很枯燥，本章我們通過選舉一些數學基礎裡容易替代的一些概念做以介紹，幫助大家更好的理清這些易被概念之間的關係。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4WKxONzFYrMU"
   },
   "source": [
    "## 1.1 向量和矩陣\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ukHRogYHYrOw"
   },
   "source": [
    "### 1.1.1 標量、向量、矩陣、張量之間的聯繫\n",
    "**標量（scalar）**\n",
    "一個標量表示一個單獨的數，它不同於線性代數中研究的其他大部分對象（通常是多個數的數組）。我們用斜體表示標量。標量通常被賦予小寫的變量名稱。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8JS5D-k-YrQu"
   },
   "source": [
    "**向量（vector）**\n",
    "​一個向量表示一組有序排列的數。通過次序中的索引，我們可以確定每個單獨的數。通常我們賦予向量粗體的小寫變量名稱，比如$x$。向量中的元素可以通過帶腳標的斜體表示。向量$X$的第一個元素是$X_1$，第二個元素是$X_2$，以此類推。我們也會註明存儲在向量中的元素的類型（實數、虛數等）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "akjq8zKAYrS9"
   },
   "source": [
    "**矩陣（matrix）**\n",
    "​矩陣是具有相同特徵和緯度的對象的集合，表現為一張二維數據表。其意義是一個對象表示為矩陣中的一個row，一個特徵表示為矩陣中的一個column，每個特徵都有數值型的取值。通常會賦予矩陣粗體的大寫變量名稱，比如$A$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBsPTbQOYrVg"
   },
   "source": [
    "**張量（tensor）**\n",
    "​在某些情況下，我們會討論坐標超過兩維的數組。一般地，一個數組中的元素分佈在若干維坐標的規則網格中，我們將其稱之為張量。使用 $A$ 來表示張量“$A$”。張量$A$中坐標為$(i,j,k)$的元素記作$A_{(i,j,k)}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qTY4n4_fYrXi"
   },
   "source": [
    "**四者之間關係**\n",
    "\n",
    "**scalar** 是 0階 **tensor**，**vector** 是一階 **tensor**。<br>\n",
    "Example:<br>\n",
    ">**scalar** 就是知道棍子的長度，但是你不會知道棍子指向哪。<br>\n",
    ">**​vector** 就是不但知道棍子的長度，還知道棍子指向前面還是後面。<br>\n",
    ">​**tensor** 就是不但知道棍子的長度，也知道棍子指向前面還是後面，還能知道這棍子又向上/下和左/右偏轉了多少。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1sI1FuhmYrZv"
   },
   "source": [
    "### 1.1.2 張量與矩陣的區別"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dIfwWblpYrcM"
   },
   "source": [
    "矩陣\n",
    "- 從代數角度講， 矩陣它是向量的推廣。向量可以看成一維的“表格”（即分量按照順序排成一排），矩陣是二維的“表格”（分量按照縱橫位置排列）， 那麼$n$階張量就是所謂的$n$維的“表格”。\n",
    "- 從幾何角度講， 矩陣是一個真正的幾何量，也就是說，它是一個不隨參照系的坐標變換而變化的東西。向量也具有這種特性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7-8ZOh7zYrea"
   },
   "source": [
    "張量：嚴格定義是利用線性映射來描述。\n",
    "- 張量可以用3×3矩陣形式來表達。\n",
    "- 表示標量的數和表示向量的三維數組也可分別看作1×1，1×3的矩陣。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aCdvY2sjYrgp"
   },
   "source": [
    "### 1.1.3 矩陣和向量相乘結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lI4N7v89Yri2"
   },
   "source": [
    "若使用愛因斯坦求和約定（Einstein summation convention），矩陣$A$, $B$相乘得到矩陣$C$可以用下式表示：\n",
    "$$ a_{ik}*b_{kj}=c_{ij} \\tag{1.3-1} $$\n",
    "其中，$a_{ik}$, $b_{kj}$, $c_{ij}$分別表示矩陣$A, B, C$的元素，$k$出現兩次，是一個啞變量（Dummy Variables）表示對該參數進行遍歷求和。\n",
    "而矩陣和向量相乘可以看成是矩陣相乘的一個特殊情況，例如：矩陣$B$是一個$n \\times 1$的矩陣。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CnevfkqkcTPY"
   },
   "source": [
    "### 1.1.4 向量和矩陣的範數歸納"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X7oo5hw7cTRZ"
   },
   "source": [
    "定義一個向量為：$\\vec{a}=[-5, 6, 8, -10]$。任意一組向量設為 $\\vec{x}=(x_1,x_2,...,x_N)$。其不同範數求解如下：\n",
    "\n",
    "- 向量的1範數：向量的各個元素的絕對值之和，上述向量 $\\vec{a}$ 的 1 範數結果就是：29。\n",
    "  \n",
    "$$\n",
    "\\Vert\\vec{x}\\Vert_1=\\sum_{i=1}^N\\vert{x_i}\\vert\n",
    "$$\n",
    "\n",
    "- 向量的2範數：向量的每個元素的平方和再開平方根，上述 $\\vec{a}$ 的 2 範數結果就是：15。\n",
    "  \n",
    "$$\n",
    "\\Vert\\vec{x}\\Vert_2=\\sqrt{\\sum_{i=1}^N{\\vert{x_i}\\vert}^2}\n",
    "$$\n",
    "\n",
    "- 向量的負無窮範數：向量的所有元素的絕對值中最小的：上述向量 $\\vec{a}$ 的負無窮範數結果就是：5。\n",
    "  \n",
    "$$\n",
    "\\Vert\\vec{x}\\Vert_{-\\infty}=\\min{|{x_i}|}\n",
    "$$\n",
    "\n",
    "- 向量的正無窮範數：向量的所有元素的絕對值中最大的：上述向量 $\\vec{a}$ 的正無窮範數結果就是：10。\n",
    "\n",
    "- 向量的p範數：\n",
    "  \n",
    "$$\n",
    "L_p=\\Vert\\vec{x}\\Vert_p=\\sqrt[p]{\\sum_{i=1}^{N}|{x_i}|^p}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "17ugIrjEcTTt"
   },
   "source": [
    "**矩陣的範數 (norm)**\n",
    "\n",
    "定義一個矩陣 $A=[[-1, 2, -3], [4, -6, 6]]$。任意矩陣定義為：$A_{m\\times n}$，其元素為 $a_{ij}$。\n",
    "\n",
    "矩陣的範數 (norm) 定義為\n",
    "\n",
    "$$\n",
    "\\Vert{A}\\Vert_p :=\\sup_{x\\neq 0}\\frac{\\Vert{Ax}\\Vert_p}{\\Vert{x}\\Vert_p}\n",
    "$$\n",
    "\n",
    "當向量取不同範數時, 相應得到了不同的矩陣範數。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bu_yu8A3cTWG"
   },
   "source": [
    "- **矩陣的 1 範數（column norm）**：矩陣的每一個 column 上的元素絕對值先求和，再從中取個最大的,（列和最大），上述矩陣$A$的1範數先得到$[5,8,9]$，再取最大的最終結果就是：9 。\n",
    "$$\n",
    "\\Vert A\\Vert_1=\\max_{1\\le j\\le n}\\sum_{i=1}^m|{a_{ij}}|\n",
    "$$\n",
    "\n",
    "- **矩陣的 2 範數**：矩陣 $A^TA$ 的最大特徵值開平方根，上述矩陣 $A$ 的 2 範數得到的最終結果是：10.0623。\n",
    "  \n",
    "$$\n",
    "\\Vert A\\Vert_2=\\sqrt{\\lambda_{max}(A^T A)}\n",
    "$$\n",
    "\n",
    "其中， $\\lambda_{max}(A^T A)$ 為 $A^T A$ 的特徵值絕對值的最大值。\n",
    "\n",
    "- **矩陣的無窮範數（row norm）**：矩陣的每一個 row 上的元素絕對值先求和，再從中取個最大的，（row 的和最大），上述矩陣 $A$ 的 row 範數先得到 $[6, 16]$，再取最大的最終結果就是 16。\n",
    "$$\n",
    "\\Vert A\\Vert_{\\infty}=\\max_{1\\le i \\le m}\\sum_{j=1}^n |{a_{ij}}|\n",
    "$$\n",
    "\n",
    "- **矩陣的核範數**：矩陣的奇異值（將矩陣svd分解）之和，這個範數可以用來矩陣的 trace 和來表示（因為最小化核範數，相當於最小化矩陣的秩 - 低秩?），上述矩陣A最終結果就是：10.9287。\n",
    "$$\n",
    "\\Vert A\\Vert=\\sqrt{tr(A^T A)}\n",
    "$$\n",
    "\n",
    "- **矩陣的 L0 範數**：矩陣的非 0 元素的個數，通常用它來表示稀疏，L0 範數越小 0 元素越多，也就越稀疏，上述矩陣 $A$ 最終結果就是：6。\n",
    "\n",
    "- **矩陣的 L1 範數**：矩陣中的每個元素絕對值之和，它是L0範數的最優凸近似，因此它也可以表示稀疏，上述矩陣 $A$ 最終結果就是：22 。\n",
    "\n",
    "- **矩陣的 F 範數**：矩陣的各個元素平方之和再開平方根，它通常也叫做矩陣的L2範數，它的優點在於它是一個凸函數，可以求導求解，易於計算，上述矩陣A最終結果就是：10.0995。\n",
    "  \n",
    "$$\n",
    "\\Vert A\\Vert_F=\\sqrt{(\\sum_{i=1}^m\\sum_{j=1}^n{| a_{ij}|}^2)}\n",
    "$$\n",
    "\n",
    "- **矩陣的L21範數**：矩陣先以每一個 column 為單位，求每一個 row 的 F 範數（也可認為是向量的 2 範數），然後再將得到的結果求 L1 範數（也可認為是向量的 1 範數），很容易看出它是介於 L1 和 L2 之間的一種範數，上述矩陣 $A$ 最終結果就是：17.1559。\n",
    "\n",
    "- **矩陣的 p範數**\n",
    "  \n",
    "$$\n",
    "\\Vert A\\Vert_p=\\sqrt[p]{(\\sum_{i=1}^m\\sum_{j=1}^n{| a_{ij}|}^p)}\n",
    "$$\n",
    "$$\n",
    "\\Vert\\vec{x}\\Vert_{+\\infty}=\\max{|{x_i}|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZUAVLHscTYV"
   },
   "source": [
    "### 1.1.5 如何判斷一個矩陣為正定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pMG5riujcTax"
   },
   "source": [
    "判定一個矩陣是否為正定，通常有以下幾個方面：\n",
    "\n",
    "- 矩陣順序主子式全大於 0\n",
    "- 存在可逆矩陣 $C$ 使 $C^TC$ 等於該矩陣\n",
    "- 正慣性指數等於 $n$\n",
    "- 合同於單位矩陣 $E$（即：規範形為 $E$）\n",
    "- 標準形中主對角元素全為正\n",
    "- 特徵值全為正\n",
    "- 是某基的度量矩陣"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-NT_tlfcTdE"
   },
   "source": [
    "## 1.2 導數和偏導數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvvDpkPlcTfd"
   },
   "source": [
    "### 1.2.1 導數偏導計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JfRQLTE-cThq"
   },
   "source": [
    "**導數定義**:\n",
    "\n",
    "導數(derivative)代表了在自變量變化趨於無窮小的時候，函數值的變化與自變量的變化的比值。幾何意義是這個點的切線。物理意義是該時刻的（瞬時）變化率。\n",
    "​\n",
    "\n",
    "*Note*：在一維函數中，只有一個自變量變動，也就是說只存在一個方向的變化率，這也就是為什麼一維函數沒有偏導數的原因。在物理學中有平均速度和瞬時速度之說。平均速度：\n",
    "\n",
    "$$\n",
    "v=\\frac{s}{t}\n",
    "$$\n",
    "\n",
    "其中 $v$ 表示平均速度，$s$ 表示路程，$t$ 表示時間。這個公式可以改寫為\n",
    "\n",
    "$$\n",
    "\\bar{v}=\\frac{\\Delta s}{\\Delta t}=\\frac{s(t_0+\\Delta t)-s(t_0)}{\\Delta t}\n",
    "$$\n",
    "\n",
    "其中 $\\Delta s$ 表示兩點之間的距離，而 $\\Delta t$ 表示走過這段距離需要花費的時間。當 $\\Delta t$ 趨向於 0（$\\Delta t \\to 0$）時，也就是時間變得很短時，平均速度也就變成了在 $t_0$ 時刻的瞬時速度，表示成如下形式：\n",
    "\n",
    "$$\n",
    "v(t_0)=\\lim_{\\Delta t \\to 0}{\\bar{v}}=\\lim_{\\Delta t \\to 0}{\\frac{\\Delta s}{\\Delta t}}=\\lim_ {\\Delta t \\to 0}{\\frac{s(t_0+\\Delta t)-s(t_0)}{\\Delta t}}\n",
    "$$\n",
    "\n",
    "實際上，上式表示的是路程 $s$ 關於時間 $t$ 的函數在 $t=t_0$ 處的導數。一般的，這樣定義導數：如果平均變化率的極限存在，即有\n",
    "\n",
    "$$\n",
    "\\lim_{\\Delta x \\to 0}{\\frac{\\Delta y}{\\Delta x}}=\\lim_{\\Delta x \\to 0}{\\frac{f(x_0+\\Delta x)-f(x_0 )}{\\Delta x}}\n",
    "$$\n",
    "\n",
    "則稱此極限為函數 $y=f(x)$ 在點 $x_0$ 處的導數。記作 $f'(x_0)$ 或 $y'\\vert_{x=x_0}$ 或 $\\frac{dy}{dx}\\vert_{x=x_0}$ 或 $\\frac{df(x)}{ dx}\\vert_{x=x_0}$。\n",
    "\n",
    "總之，導數就是曲線在某一點切線的斜率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iMEDRUwncTkD"
   },
   "source": [
    "**偏導數**:\n",
    "\n",
    "既然談到偏導數(partial derivative)，那就至少涉及到兩個自變量。以兩個自變量為例，$z=f(x,y)$，從導數到偏導數，也就是從曲線來到了曲面。曲線上的一點，其切線只有一條。但是曲面上的一點，切線有無數條。而偏導數就是指多元函數沿著坐標軸的變化率。\n",
    "\n",
    "\n",
    "*注意*：直觀地說，偏導數也就是函數在某一點上沿坐標軸正方向的的變化率。\n",
    "\n",
    "設函數 $z=f(x,y)$ 在點 $(x_0,y_0)$ 的領域內有定義，當 $y=y_0$ 時，$z$ 可以看作關於 $x$ 的一元函數 $f(x,y_0)$，若該一維函數在 $x=x_0$ 處可導，即有函數的極限$A$存在。那麼稱 $A$ 為函數 $z=f(x,y)$ 在點 $(x_0,y_0)$ 處關於自變量 $x$ 的偏導數，記作 $f_x(x_0,y_0)$ 或 $\\frac{\\partial z}{\\partial x}\\vert_{y=y_0}^{x=x_0}$ 或 $\\frac{\\partial f}{\\partial x}\\vert_{y=y_0}^{x= x_0}$ 或 $z_x\\vert_{y=y_0}^{x=x_0}$。\n",
    "\n",
    "$$\n",
    "A=\\lim_{\\Delta x \\to 0}{\\frac{f(x_0+\\Delta x,y_0)-f(x_0,y_0)}{\\Delta x}}\n",
    "$$\n",
    "\n",
    "\n",
    "偏導數在求解時可以將另外一個變量看做常數，利用普通的求導方式求解，比如 $z=3x^2+xy$ 關於 $x$ 的偏導數就為 $z_x=6x+y$，這個時候 $y$ 相當於 $x$ 的係數。\n",
    "\n",
    "某點 $(x_0,y_0)$ 處的偏導數的幾何意義為曲面 $z=f(x,y)$ 與面 $x=x_0$ 或面 $y=y_0$ 交線在 $y=y_0$ 或 $x=x_0$ 處切線的斜率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uudkQSh3cTmZ"
   },
   "source": [
    "### 1.2.2 導數和偏導數有什麼區別？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IQbAs7ZzcTok"
   },
   "source": [
    "導數和偏導沒有本質區別，如果極限存在，都是當自變量的變化量趨於 0 時，函數值的變化量與自變量變化量比值的極限。\n",
    "\n",
    "> - 一維函數，一個 $y$ 對應一個 $x$，導數只有一個。\n",
    "> - 二維函數，一個 $z$ 對應一個 $x$ 和一個 $y$，有兩個導數：一個是 $z$ 對 $x$ 的導數，一個是 $z$ 對 $y$ 的導數，稱之為偏導。\n",
    "> - 求偏導時要注意，對一個變量求導，則視另一個變量為常數，只對改變量求導，從而將偏導的求解轉化成了一維函數的求導。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EtmooRfXcTrQ"
   },
   "source": [
    "## 1.3 特徵值和特徵向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWDA1rRLcTtz"
   },
   "source": [
    "### 1.3.1 特徵值分解與特徵向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RrMI1wwacTwG"
   },
   "source": [
    "- 特徵值分解可以得到特徵值 (eigenvalues) 與特徵向量 (eigenvectors)\n",
    "- 特徵值表示的是這個特徵到底有多重要，而特徵向量表示這個特徵是什麼。\n",
    "\n",
    "如果說一個向量 $\\vec{v}$ 是方陣 $A$ 的特徵向量，將一定可以表示成下面的形式：\n",
    "$$\n",
    "A\\nu = \\lambda \\nu\n",
    "$$\n",
    "\n",
    "$\\lambda$ 為特徵向量 $\\vec{v}$ 對應的特徵值。特徵值分解是將一個矩陣分解為如下形式：  \n",
    "$$\n",
    "A=Q\\sum Q^{-1}\n",
    "$$\n",
    "\n",
    "其中，$Q$ 是這個矩陣 $A$ 的特徵向量組成的矩陣，$\\sum$ 是一個對角矩陣，每一個對角線元素就是一個特徵值，裡面的特徵值是由大到小排列的，這些特徵值所對應的特徵向量就是描述這個矩陣變化方向（從主要的變化到次要的變化排列）。也就是說矩陣 $A$ 的信息可以由其特徵值和特徵向量表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AeMZELetcTyR"
   },
   "source": [
    "### 1.3.2 奇異值與特徵值有什麼關係"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qxVcnULZcT0y"
   },
   "source": [
    "那麼奇異值和特徵值是怎麼對應起來的呢？我們將一個矩陣$A$的轉置乘以$A$，並對$A^TA$求特徵值，則有下面的形式：\n",
    "\n",
    "$$\n",
    "(A^TA)V = \\lambda V\n",
    "$$\n",
    "\n",
    "這裡 $V$ 就是上面的右奇異向量，另外還有：\n",
    "\n",
    "$$\n",
    "\\sigma_i = \\sqrt{\\lambda_i}, u_i=\\frac{1}{\\sigma_i}A\\mu_i\n",
    "$$\n",
    "\n",
    "這裡的 $\\sigma$ 就是奇異值，$u$就是上面說的左奇異向量。<br>\n",
    "奇異值 $\\sigma$ 跟特徵值類似，在矩陣 $\\sum$ 中也是從大到小排列，而且 $\\sigma$ 的減少特別的快，在很多情況下，前 10% 甚至 1% 的奇異值的和就佔了全部的奇異值之和的 99% 以上了。<br>\n",
    "也就是說，我們也可以用前 $r$（ $r$遠小於$m, n$）個的奇異值來近似描述矩陣，即部分奇異值分解：\n",
    "$$\n",
    "A_{m\\times n}\\approx U_{m \\times r}\\sum_{r\\times r}V_{r \\times n}^T\n",
    "$$\n",
    "\n",
    "右邊的三個矩陣相乘的結果將會是一個接近於 $A$ 的矩陣，在這邊 $r$ 越接近於 $n$，則相乘的結果越接近於 $A$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2nMyhrRvcT28"
   },
   "source": [
    "## 1.4 機率分佈與隨機變量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rvGcFm88cT7j"
   },
   "source": [
    "### 1.4.1 機器學習為什麼要使用機率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ax4ZITaicT9r"
   },
   "source": [
    "事件的機率是衡量該事件發生的可能性的量度。雖然在一次隨機試驗中某個事件的發生是帶有偶然性的，但那些可在相同條件下大量重複的隨機試驗卻往往呈現出明顯的數量規律。\n",
    ">- 機器學習除了處理不確定量，也需處理隨機量。不確定性和隨機性可能來自多個方面，使用機率論來量化不確定性。<br>\n",
    ">- 機率論在機器學習中扮演著一個核心角色，因為機器學習算法的設計通常依賴於對數據的機率假設。<br>\n",
    ">- 例如在機器學習（Andrew Ng）的課中，會有一個單純貝氏分類器假設就是條件獨立的一個例子。該學習算法對內容做出假設，用來分辨電子郵件是否為垃圾郵件。假設無論郵件是否為垃圾郵件，單詞 $x$ 出現在郵件中的機率條件獨立於單詞 $y$ 。很明顯這個假設不是 unbiased 的，因為某些單字幾乎總是同時出現。然而，最終結果是，這個簡單的假設對結果的影響並不大，且無論如何都可以讓我們快速判別垃圾郵件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0epudqHZcT_3"
   },
   "source": [
    "### 1.4.2 變量與隨機變量有什麼區別"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j8fFogdBcUCL"
   },
   "source": [
    "**隨機變量**（random variable）\n",
    "\n",
    "表示隨機現象（在一定條件下，並不總是出現相同結果的現象稱為隨機現象）中各種結果的實值函數（一切可能的樣本點）。例如某一時間內公共汽車站等車乘客人數，電話交換台在一定時間內收到的呼叫次數等，都是隨機變量的實例。<br>\n",
    "隨機變量與模糊變量的不確定性的本質差別在於，後者的測定結果仍具有不確定性，即模糊性。\n",
    "\n",
    "**變量與隨機變量的區別：**\n",
    "當變量的取值的機率不是 1 時，變量就變成了隨機變量；當隨機變量取值的機率為 1 時，隨機變量就變成了變量。\n",
    "\n",
    "> Example：\n",
    "> - 當變量 $x$ 值為 100 的機率為 1 的話,那麼 $x=100$ 就是確定條件，不會再有變化，除非有進一步的運算\n",
    "> - 當變量 $x$ 值為 100 的機率為 1，比如為 50 的機率是 0.5，為 100 的機率也還是 0.5，那麼這個變量就是會隨不同條件而變化的，是隨機變量，取到 50 或者 100 的機率都是 0.5，即50%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hCDashfkcUE6"
   },
   "source": [
    "### 1.4.3 隨機變量與機率分佈的關聯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gu5YFQ_kcUHB"
   },
   "source": [
    "一個隨機變量僅表示一個可能取得的狀態，還必須給定與之相伴的機率分佈來製定每個狀態的可能性。<br>\n",
    "用來描述隨機變量或一簇隨機變量的每一個可能的狀態的可能性大小的方法就是 **機率分佈(probability distribution)**.\n",
    "\n",
    "隨機變量可以分為 Discrete Random Variable 和 Continuous Random Variable。\n",
    "\n",
    "相應的描述其機率分佈的函數是\n",
    "\n",
    "機率質量函數(Probability Mass Function, PMF)：描述離散型隨機變量的機率分佈，通常用大寫字母 $P$ 表示。\n",
    "\n",
    "機率密度函數(Probability Density Function, PDF)：描述連續型隨機變量的機率分佈，通常用小寫字母 $p$ 表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CxMXprI5cUJo"
   },
   "source": [
    "### 1.4.4 離散型隨機變量 和 機率質量函數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NiY6cmhXcULy"
   },
   "source": [
    "PMF：將隨機變量所能夠取得的每個狀態投影到隨機變量，並取得該狀態的機率。\n",
    "\n",
    "- 一般而言，$P(x)$ 表示時 $X=x$ 的機率<br>\n",
    "- 有時候為了防止混淆，要明確寫出隨機變量的名稱 $P(X=x)$<br>\n",
    "- 有時候需要先定義一個隨機變量，然後製定它遵循的機率分佈 $x$ 服從 $P(x)$<br>\n",
    "\n",
    "PMF：可以同時作用於多個隨機變量，即聯合機率分佈(joint probability distribution) $P(X=x,Y=y)$ 表示 $X=x$ 和 $Y=y$ 同時發生的機率，也可以簡寫成 $P(x,y)$\n",
    "\n",
    "如果一個函數 $P$ 是隨機變量 $X$ 的 PMF，那麼它必須滿足如下三個條件\n",
    "\n",
    "- $P$ 的定義域必須是的所有可能狀態的集合<br>\n",
    "- $\\forall x \\in x, 0 \\leq P(x) \\leq 1 $\n",
    "- $\\sum_{x \\in X} P(x) = 1$。 我們把這一條性質稱之為：標準化(normalized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FPjvqpR2cUN3"
   },
   "source": [
    "### 1.4.5 連續型隨機變量 和 機率密度函數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gbEBX1RLcUQM"
   },
   "source": [
    "如果一個函數 $p$ 是 x 的 PDF，那麼它必須滿足如下幾個條件\n",
    "\n",
    "- $p$ 的定義域必須是 x 的所有可能狀態的集合。<br>\n",
    "- $\\forall x \\in X, p(x) \\ge 0$<br>\n",
    "Note：我們並不要求 $p(x) \\le 1$，因為此處 $p(x)$ 不是表示的對應此狀態具體的機率，而是機率的一個相對大小(密度)。具體的機率要用積分去求。<br>\n",
    "- $\\int p(x)dx=1$，積分下來，總和還是 1，機率和還是 1\n",
    "\n",
    "Note：PDF $p(x)$ 並沒有直接對特定的狀態給出機率，給出的是密度，相對的，它給出了落在面積為 $\\delta_{x}$ 的無線小區域內機率為 $ p(x) \\delta_{x}$。由此，雖然我們無法求得具體某個狀態的機率，但我們可以得到某個狀態 $x$ 落在某個區間 $[a,b]$ 內的機率為 $ \\int_{a}^{b}p(x)dx$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.6 條件機率舉例說明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "條件機率的公式如下：\n",
    "$$\n",
    "P(A\\mid B) = \\frac{P(A\\cap B)}{P(B)}\n",
    "$$\n",
    "說明：在同一個樣本空間 $\\Omega$ 中的事件或者子集 $A$ 與 $B$，如果隨機從 $\\Omega$ 中選出的一個元素屬於 $B$，那麼下一個隨機選擇的元素屬於$A$ 的機率就定義為在 $B$ 的前提下 $A$ 的條件機率。<br>\n",
    "\n",
    "![條件機率](./img/ch1/conditional_probability.jpg)\n",
    "\n",
    "圖1.1 條件機率文氏圖\n",
    "\n",
    "根據文氏圖，可以很清楚地看到在事件 B 發生的情況下，事件 A 發生的機率就是 $\\frac {P(A\\cap B)}{P(B)}$。<br>\n",
    "\n",
    "Example：一對夫妻有兩個小孩，已知其中一個是女孩，則另一個是女孩子的機率是多少？ （面試、筆試可能會碰到）<br>\n",
    "- **窮舉法**：已知其中一個是女孩，那麼樣本空間為男女，女女，女男，則另外一個仍然是女生的機率就是 $\\frac{1}{3}$<br>\n",
    "- **條件機率法**：$P(Female\\mid Female)=\\frac{P(Female\\cap Female)}{P(Female)}$，夫妻有兩個小孩，那麼它的樣本空間為女女，男女，女男，男男，則 $P(Female \\cap Female)$ 為$\\frac{1}{4}$，$P(Female)= 1-P(Male \\cap Male)=\\frac{3}{4}$，所以最後 $\\frac{1}{3}$。<br>\n",
    "\n",
    "這里大家可能會誤會 男女 和 女男 是同一種情況，但實際上姐弟和兄妹便是不同情況。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.7 聯合(joint)機率 與 邊際(margin)機率 關聯與區別"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Difference：**\n",
    "- 聯合機率：聯合機率指類似於 $P(X=a,Y=b)$ 這樣，包含多個條件，且所有條件同時成立的機率。聯合機率是指在多元的機率分佈中多個隨機變量分別滿足各自條件的機率。<br>\n",
    "- 邊際機率：邊際機率是某個事件發生的機率，而與其它事件無關。邊際機率指類似於$P(X=a)$，$P(Y=b)$這樣，僅與單個隨機變量有關的機率。\n",
    "\n",
    "**Common：**\n",
    "- 聯合分佈可求邊際分佈，但若只知道邊際分佈，無法求得聯合分佈。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.8 條件機率的公式與原則"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由條件機率的定義，可直接得出下面的乘法公式：\n",
    "乘法公式：假設 $A, B$是兩個事件，並且$P(A) > 0$, 則：\n",
    "$$\n",
    "P(AB) = P(B\\mid A) P(A)\n",
    "$$\n",
    "Extend：\n",
    "$$\n",
    "P(ABC)=P(C\\mid AB)P(B\\mid A)P(A)\n",
    "$$\n",
    "一般來說可以用歸納法求證：<br>\n",
    "若$P(A_1A_2...A_n)>0$，則有\n",
    "$$\n",
    "P(A_1A_2...A_n)=P(A_n\\mid A_1A_2...A_{n-1})P(A_{n-1}\\mid A_1A_2...A_{n-2})...P(A_2\\mid A_1)P(A_1)\n",
    "=P(A_1)\\prod_{i=2}^{n}P(A_i\\mid A_1A_2...A_{i-1})\n",
    "$$\n",
    "任何多維隨機變量的聯合機率分佈，都可以分解成只有一個變量的條件機率相乘形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.9 獨立性和條件獨立性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**獨立性：**\n",
    "- 兩個隨機變量 $x$ 和 $y$，其機率分佈表示成兩個因子相乘形式，一個因子只包含 $x$，另一個因子只包含 $y$，兩個隨機變量相互獨立(independent)。<br>\n",
    "- 條件有時為不獨立的事件之間帶來獨立，有時也會因為此條件的存在，而讓本來獨立的事件失去獨立性。<br>\n",
    "Example：$P(XY)=P(X)P(Y)$, 事件 $X$ 和事件 $Y$ 獨立。此時給定 $Z$ 使得 $P(X,Y\\mid Z) \\not = P(X\\mid Z)P(Y\\mid Z)$ <br>\n",
    "\n",
    "事件獨立時，聯合機率等於機率的乘積。這是一個非常好的數學性質，然而不幸的是，無條件的獨立是十分稀少的，因為大部分情況下，事件之間都是互相影響的。\n",
    "\n",
    "**條件獨立性：**\n",
    "- 給定 $Z$ 的情況下, $X$ 和 $Y$ 相互獨立，當且僅當 $X$ 和 $Y$ 的關係依賴於 $Z$，而不是直接產生。\n",
    "\n",
    "$$\n",
    "X\\bot Y\\mid Z \\iff P(X\\cap Y\\mid Z) = P(X\\mid Z)P(Y\\mid Z)\n",
    "$$\n",
    "\n",
    "**Example**：定義如下事件：\n",
    "> $X$：明天下雨<br>\n",
    "> $Y$：今天的地面是濕的<br>\n",
    "> $Z$：今天是否下雨<br>\n",
    "> $Z$ 事件的成立，對 $X$ 和 $Y$ 均有影響，然而，在 $Z$ 事件成立的前提下，今天的地面情況對明天是否下雨沒有影響。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 常見機率分佈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 Bernoulli分佈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bernoulli分佈**是單個二值隨機變量分佈, 單參數 $\\phi \\in [0,1]$ 控制，$\\phi$ 給出隨機變量等於 1 的機率。\n",
    "\n",
    "主要性質有:\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(x=1) &= \\phi \\\\\n",
    "P(x=0) &= 1-\\phi \\\\\n",
    "P(x=x) &= \\phi^x(1-\\phi)^{1-x} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "期望值 和 變異數：\n",
    "$$\n",
    "\\begin{align*}\n",
    "E_x[x] &= \\phi \\\\\n",
    "Var_x(x) &= \\phi{(1-\\phi)}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinoulli分佈**也叫**範疇分佈**, 是單個 k 值隨機分佈，經常用來表示**對象分類的分佈**。\n",
    "\n",
    "其中 $k$ 是有限值，Multinoulli 分佈由向量 $ \\vec{p}\\in[0,1]^{k-1}$ 參數化，每個分量 $p_i$ 表示第 $i$ 個狀態的機率，且 $p_k=1-1^{T}p$。<br>\n",
    "Note：\n",
    "- $1^{T}p \\le 1$<br>\n",
    "- $p = <p_1, p_2, ..., p_i, ..., p_n>$\n",
    "\n",
    "**適用範圍**: **Bernoulli分佈**適合對**離散型**隨機變量建模。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 高斯分佈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高斯分布也叫正態分佈(Normal Distribution)，其機率函數如下:\n",
    "$$\n",
    "x\\sim N(\\mu,\\sigma^2) = \\sqrt{\\frac{1}{2\\pi\\sigma^{2}}}exp\\left ( -\\frac{(x -\\mu)^{2}}{2\\sigma^2} \\right )\n",
    "$$\n",
    "其中，$\\mu$ 和 $\\sigma$ 分別是均值和標準差, 中心峰值 $x$ 坐標為由 $\\mu$, 峰的寬度受 $\\sigma$ 控制, 最大點在 $x=\\mu$ 處取得，反曲點\n",
    "為$x=\\mu\\pm\\sigma$\n",
    "\n",
    "正態分佈中，±1$\\sigma$、±2$\\sigma$、±3$\\sigma$下的機率分別是68.3%、95.5%、99.73% (最好記住)\n",
    "\n",
    "此外，令$\\mu=0,\\sigma=1$ 高斯分佈即簡化為標準正態分佈:\n",
    "$$\n",
    "x\\sim N(\\mu,\\sigma^2) = \\sqrt{\\frac{1}{2\\pi}}exp\\left(-\\frac{x^2}{2}\\right )\n",
    "$$\n",
    "\n",
    "其中，$\\beta=\\frac{1}{\\sigma^2}$ 通過參數 $\\beta \\in (0, \\infty)$ 來控制分佈精準度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.3 何時會使用正態分佈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 何時使用正態分佈？<br>\n",
    "A: 缺乏實數上分佈的先覺知識、不知選擇何種形式時，默認選擇正態分佈總是不會錯的，理由如下:\n",
    "\n",
    "1. 中心極限定理告訴我們, 很多獨立隨機變量均近似於正態分佈，現實中很多複雜系統都可以被建模成正態分佈的噪聲, 即使該系統可以被結構化分解。\n",
    "2. 正態分佈是具有相同方差的所有機率分佈中不確定性最大的分佈，換句話說，正態分佈是對模型加入先覺知識最少的分佈。\n",
    "\n",
    "正態分佈的延伸:\n",
    "正態分佈可以延伸到 $R^{n}$ 空間，此時稱為**高維正態分佈**，其參數是一個正定對稱矩陣 $\\Sigma$：\n",
    "$$\n",
    "x\\sim N(\\vec\\mu,\\Sigma)=\\sqrt{\\frac{1}{(2\\pi)^{n} det(\\Sigma)}}exp\\left(-\\frac{(\\vec{x}-\\vec{\\mu})^T\\Sigma^{-1}(\\vec{x}-\\vec{\\mu})}{2}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.4 指數分佈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度學習中，指數分佈用來描述在 $x=0$ 點處取得邊界點的分佈，指數分佈定義如下:\n",
    "$$\n",
    "x\\sim p(\\lambda)=\\lambda I_{x\\geq 0}exp(-\\lambda{x})\n",
    "$$\n",
    "指數分佈用指示函數 $I_{x\\geq 0}$ 來使 $x$ 取負值時的機率為 $0$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.5 Laplace 分佈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplace 分佈（Laplace distribution）：一個聯繫緊密的機率分佈，它允許我們在任意一點 $\\mu$ 處設置機率質量的峰值\n",
    "\n",
    "$$\n",
    "x\\sim Laplace(\\mu, \\gamma)=\\frac{1}{2\\gamma}exp\\left(-\\frac{|x-\\mu|}{\\gamma}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.6 Dirac分佈和經驗分佈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dirac 分佈可保證機率分佈中所有的質量都集中在一個點上。<br>\n",
    "Dirac 分佈的狄拉克$\\delta$函數 (也稱為**單位脈衝函數**) 定義如下:\n",
    "$$\n",
    "p(x)=\\delta(x-\\mu), x\\neq \\mu\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\int_{a}^{b}\\delta(x-\\mu)dx = 1, a < \\mu < b\n",
    "$$\n",
    "\n",
    "Dirac 分佈 經常作為 經驗分佈（empirical distribution）的一個組成部分出現\n",
    "$$\n",
    "\\hat{p}(\\vec{x})=\\frac{1}{m}\\sum_{i=1}^{m}\\delta(\\vec{x}-{\\vec{x}}^{ (i)})\n",
    "$$\n",
    "\n",
    "![經驗分佈](./img/ch1/Empirical_CDF.png)\n",
    "\n",
    "圖二. 經驗分佈\n",
    "\n",
    "其中，m 個點 $x^{1},...,x^{m}$ 是給定的數據集, **經驗分佈** 將機率密度 $\\frac{1}{m}$ 賦給了這些點。\n",
    "\n",
    "當我們在訓練集上訓練模型時，可以認為從這個訓練集上得到的經驗分佈指明了**取樣來源**.\n",
    "\n",
    "**適用範圍**: 狄拉克$\\delta$ 函數適合對**連續型**隨機變量的經驗分佈."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 期望值、變異數、共變異數 (Covariance)、相關係數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1 期望值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在機率論和統計學中，數學期望值（或均值）是實驗中每次可能結果的機率乘以其結果的總和。它反映隨機變量平均取值的大小。\n",
    "\n",
    "- 線性運算： $E(ax+by+c) = aE(x)+bE(y)+c$\n",
    "- 延伸形式： $E(\\sum_{k=1}^{n}{a_ix_i+c}) = \\sum_{k=1}^{n}{a_i E(x_i)+c}$\n",
    "- 函數期望值：假設 $f(x)$ 為 $x$ 的函數，則 $f(x)$ 的期望值為\n",
    "    - 離散函數： $E(f(x))=\\sum_{k=1}^{n}{f(x_k)P(x_k)}$\n",
    "    - 連續函數： $E(f(x))=\\int_{-\\infty}^{+\\infty}{f(x)p(x)dx}$\n",
    "\n",
    "Note：\n",
    "> - 函數的期望值 $\\ge$ 期望值的函數（Jensen不等式），即 $E(f(x))\\geqslant f(E(x))$\n",
    "> - 一般來說，乘積的期望值不等於期望值的乘積。\n",
    "> - 如果 $X$ 和 $Y$ 相互獨立，則 $E(xy)=E(x)E(y)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2 變異數 (Variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機率論中變異數是用來衡量隨機變量和期望值之間的偏離程度。定義為：\n",
    "\n",
    "$$\n",
    "Var(x) = E((x-E(x))^2)\n",
    "$$\n",
    "\n",
    "> 性質：<br>\n",
    "> 1）$Var(x) = E(x^{2}) -E(x)^{2}$<br>\n",
    "> 2）常數函數的 Variance 為 0<br>\n",
    "> 3）Variance非線性<br>\n",
    "> 4）如果 $X$ 和 $Y$ 相互獨立，$Var(ax+by)=a^2Var(x)+b^2Var(y)$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.3 共變異數 (Covariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance 是用來衡量兩個變量的線性相關性強度及變量尺度。定義為：\n",
    "$$\n",
    "Cov(x,y)=E((x-E(x))(y-E(y)))\n",
    "$$\n",
    "\n",
    "當 $X=Y$ 時，$Cov(x,y)=Var(x)=Var(y)$\n",
    "\n",
    "> 性質：<br>\n",
    "> 1）獨立變量的 Covariance 為 0<br>\n",
    "> 2）Covariance 計算公式：\n",
    "$$\n",
    "Cov(\\sum_{i=1}^{m}{a_ix_i}, \\sum_{j=1}^{m}{b_jy_j}) = \\sum_{i=1}^{m} \\sum_{j=1 }^{m}{a_ib_jCov(x_iy_i)}\n",
    "$$\n",
    "> 3）特殊情況：\n",
    "$$\n",
    "Cov(a+bx, c+dy) = bdCov(x, y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.4 相關係數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相關係數是研究變量之間線性相關程度的量。定義為：\n",
    "$$\n",
    "Corr(x,y) = \\frac{Cov(x,y)}{\\sqrt{Var(x)Var(y)}}\n",
    "$$\n",
    "\n",
    "> 性質：<br>\n",
    "> 1）有上下界。相關係數的取值範圍是 [-1,1]。<br>\n",
    "> 2）值越接近 1，說明兩個變量正相關性（線性）越強。越接近-1，說明負相關性越強，當為0時，表示兩個變量沒有相關性。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 相關分布整理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.1\n",
    "> 單點分布(Single-Point Distribution)<br>\n",
    "> 兩點分布(Bernoulli Distribution)<br>\n",
    "> 對數分布(Logarithmic distribution)<br>\n",
    "\n",
    "![](./img/ch1/prob_distribution_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.2\n",
    "> 二項分布(Binomial Distribution)<br>\n",
    "> 幾何分布(Geometric Distribution)<br>\n",
    "> 負二項分布(Pascal Distribution)<br>\n",
    "\n",
    "![](./img/ch1/prob_distribution_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.3\n",
    "> 超幾何分布(Hypergeometric Distribution)<br>\n",
    "> 帕松分布 (Poisson distribution)<br>\n",
    "> 逆高斯分布(Inverse Gaussian Distribution)<br>\n",
    "\n",
    "![](./img/ch1/prob_distribution_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.4\n",
    "> 連續型均勻分布 (Uniform Distribution)<br>\n",
    "> 常態分布 (Normal Distribution)<br>\n",
    "> 柯西分布 (Cauchy Distribution)<br>\n",
    "\n",
    "![](./img/ch1/prob_distribution_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.5\n",
    "> 對數常態分布 (Log-normal Distribution)<br>\n",
    "> 伽瑪分布 (Gamma Distribution)<br>\n",
    "> Β分布 (Beta Distribution)<br>\n",
    "\n",
    "![](./img/ch1/prob_distribution_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.6\n",
    "> 指數分布 (Exponential Distribution)<br>\n",
    "> 韋伯分布 (Weibull Distribution)<br>\n",
    "> 拉普拉斯分布 (Laplace distribution)<br>\n",
    "\n",
    "![](./img/ch1/prob_distribution_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.7\n",
    "> 帕累托分布 (Pareto Distribution)<br>\n",
    "> 極值分佈 (Extreme Value Distribution)<br>\n",
    "> 羅吉斯分配 (Logistic Distribution)<br>\n",
    "\n",
    "![](./img/ch1/prob_distribution_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 參考文獻"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]Ian，Goodfellow，Yoshua，Bengio，Aaron...深度學習[M]，中國人民郵電出版，2017\n",
    "\n",
    "[2]周志華.機器學習[M]. 中國清華大學出版社，2016.\n",
    "\n",
    "[3]同濟大學數學系.高等數學（第七版）[M]，中國高等教育出版社，2014.\n",
    "\n",
    "[4]盛驟，試式千，潘承毅等編. 機率論與數理統計（第4版）[M]，中國高等教育出版社，2008"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMLaJ5pUWRna3Z3JBpk6C1Q",
   "collapsed_sections": [],
   "name": "第一章 數學基礎",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
