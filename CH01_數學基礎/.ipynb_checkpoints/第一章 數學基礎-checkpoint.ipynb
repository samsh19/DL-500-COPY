{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RuYmWSZ5YhQh"
   },
   "source": [
    "#第一章數學基礎\n",
    "深度學習通常又需要某些數學基礎？這些數學知識有相關性，但實際上按照這樣的知識範圍來學習，學習成本會很久，而且會很枯燥，本章我們通過選舉一些數學基礎裡容易替代的一些概念做以介紹，幫助大家更好的理清這些易被概念之間的關係。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4WKxONzFYrMU"
   },
   "source": [
    "## 1.1 向量和矩陣\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ukHRogYHYrOw"
   },
   "source": [
    "### 1.1.1 標量、向量、矩陣、張量之間的聯繫\n",
    "**標量（scalar）**\n",
    "一個標量表示一個單獨的數，它不同於線性代數中研究的其他大部分對象（通常是多個數的數組）。我們用斜體表示標量。標量通常被賦予小寫的變量名稱。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8JS5D-k-YrQu"
   },
   "source": [
    "**向量（vector）**\n",
    "​一個向量表示一組有序排列的數。通過次序中的索引，我們可以確定每個單獨的數。通常我們賦予向量粗體的小寫變量名稱，比如$x$。向量中的元素可以通過帶腳標的斜體表示。向量$X$的第一個元素是$X_1$，第二個元素是$X_2$，以此類推。我們也會註明存儲在向量中的元素的類型（實數、虛數等）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "akjq8zKAYrS9"
   },
   "source": [
    "**矩陣（matrix）**\n",
    "​矩陣是具有相同特徵和緯度的對象的集合，表現為一張二維數據表。其意義是一個對象表示為矩陣中的一個row，一個特徵表示為矩陣中的一個column，每個特徵都有數值型的取值。通常會賦予矩陣粗體的大寫變量名稱，比如$A$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBsPTbQOYrVg"
   },
   "source": [
    "**張量（tensor）**\n",
    "​在某些情況下，我們會討論坐標超過兩維的數組。一般地，一個數組中的元素分佈在若干維坐標的規則網格中，我們將其稱之為張量。使用 $A$ 來表示張量“$A$”。張量$A$中坐標為$(i,j,k)$的元素記作$A_{(i,j,k)}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qTY4n4_fYrXi"
   },
   "source": [
    "**四者之間關係**\n",
    "\n",
    "**scalar** 是 0階 **tensor**，**vector** 是一階 **tensor**。<br>\n",
    "Example:<br>\n",
    ">**scalar** 就是知道棍子的長度，但是你不會知道棍子指向哪。<br>\n",
    ">**​vector** 就是不但知道棍子的長度，還知道棍子指向前面還是後面。<br>\n",
    ">​**tensor** 就是不但知道棍子的長度，也知道棍子指向前面還是後面，還能知道這棍子又向上/下和左/右偏轉了多少。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1sI1FuhmYrZv"
   },
   "source": [
    "### 1.1.2 張量與矩陣的區別"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dIfwWblpYrcM"
   },
   "source": [
    "矩陣\n",
    "- 從代數角度講， 矩陣它是向量的推廣。向量可以看成一維的“表格”（即分量按照順序排成一排），矩陣是二維的“表格”（分量按照縱橫位置排列）， 那麼$n$階張量就是所謂的$n$維的“表格”。\n",
    "- 從幾何角度講， 矩陣是一個真正的幾何量，也就是說，它是一個不隨參照系的坐標變換而變化的東西。向量也具有這種特性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7-8ZOh7zYrea"
   },
   "source": [
    "張量：嚴格定義是利用線性映射來描述。\n",
    "- 張量可以用3×3矩陣形式來表達。\n",
    "- 表示標量的數和表示向量的三維數組也可分別看作1×1，1×3的矩陣。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aCdvY2sjYrgp"
   },
   "source": [
    "### 1.1.3 矩陣和向量相乘結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lI4N7v89Yri2"
   },
   "source": [
    "若使用愛因斯坦求和約定（Einstein summation convention），矩陣$A$, $B$相乘得到矩陣$C$可以用下式表示：\n",
    "$$ a_{ik}*b_{kj}=c_{ij} \\tag{1.3-1} $$\n",
    "其中，$a_{ik}$, $b_{kj}$, $c_{ij}$分別表示矩陣$A, B, C$的元素，$k$出現兩次，是一個啞變量（Dummy Variables）表示對該參數進行遍歷求和。\n",
    "而矩陣和向量相乘可以看成是矩陣相乘的一個特殊情況，例如：矩陣$B$是一個$n \\times 1$的矩陣。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CnevfkqkcTPY"
   },
   "source": [
    "### 1.1.4 向量和矩陣的範數歸納"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X7oo5hw7cTRZ"
   },
   "source": [
    "定義一個向量為：$\\vec{a}=[-5, 6, 8, -10]$。任意一組向量設為 $\\vec{x}=(x_1,x_2,...,x_N)$。其不同範數求解如下：\n",
    "\n",
    "- 向量的1範數：向量的各個元素的絕對值之和，上述向量 $\\vec{a}$ 的 1 範數結果就是：29。\n",
    "  \n",
    "$$\n",
    "\\Vert\\vec{x}\\Vert_1=\\sum_{i=1}^N\\vert{x_i}\\vert\n",
    "$$\n",
    "\n",
    "- 向量的2範數：向量的每個元素的平方和再開平方根，上述 $\\vec{a}$ 的 2 範數結果就是：15。\n",
    "  \n",
    "$$\n",
    "\\Vert\\vec{x}\\Vert_2=\\sqrt{\\sum_{i=1}^N{\\vert{x_i}\\vert}^2}\n",
    "$$\n",
    "\n",
    "- 向量的負無窮範數：向量的所有元素的絕對值中最小的：上述向量 $\\vec{a}$ 的負無窮範數結果就是：5。\n",
    "  \n",
    "$$\n",
    "\\Vert\\vec{x}\\Vert_{-\\infty}=\\min{|{x_i}|}\n",
    "$$\n",
    "\n",
    "- 向量的正無窮範數：向量的所有元素的絕對值中最大的：上述向量 $\\vec{a}$ 的正無窮範數結果就是：10。\n",
    "\n",
    "- 向量的p範數：\n",
    "  \n",
    "$$\n",
    "L_p=\\Vert\\vec{x}\\Vert_p=\\sqrt[p]{\\sum_{i=1}^{N}|{x_i}|^p}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "17ugIrjEcTTt"
   },
   "source": [
    "**矩陣的範數 (norm)**\n",
    "\n",
    "定義一個矩陣 $A=[[-1, 2, -3], [4, -6, 6]]$。任意矩陣定義為：$A_{m\\times n}$，其元素為 $a_{ij}$。\n",
    "\n",
    "矩陣的範數 (norm) 定義為\n",
    "\n",
    "$$\n",
    "\\Vert{A}\\Vert_p :=\\sup_{x\\neq 0}\\frac{\\Vert{Ax}\\Vert_p}{\\Vert{x}\\Vert_p}\n",
    "$$\n",
    "\n",
    "當向量取不同範數時, 相應得到了不同的矩陣範數。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bu_yu8A3cTWG"
   },
   "source": [
    "- **矩陣的 1 範數（column norm）**：矩陣的每一個 column 上的元素絕對值先求和，再從中取個最大的,（列和最大），上述矩陣$A$的1範數先得到$[5,8,9]$，再取最大的最終結果就是：9 。\n",
    "$$\n",
    "\\Vert A\\Vert_1=\\max_{1\\le j\\le n}\\sum_{i=1}^m|{a_{ij}}|\n",
    "$$\n",
    "\n",
    "- **矩陣的 2 範數**：矩陣 $A^TA$ 的最大特徵值開平方根，上述矩陣 $A$ 的 2 範數得到的最終結果是：10.0623。\n",
    "  \n",
    "$$\n",
    "\\Vert A\\Vert_2=\\sqrt{\\lambda_{max}(A^T A)}\n",
    "$$\n",
    "\n",
    "其中， $\\lambda_{max}(A^T A)$ 為 $A^T A$ 的特徵值絕對值的最大值。\n",
    "\n",
    "- **矩陣的無窮範數（row norm）**：矩陣的每一個 row 上的元素絕對值先求和，再從中取個最大的，（row 的和最大），上述矩陣 $A$ 的 row 範數先得到 $[6, 16]$，再取最大的最終結果就是 16。\n",
    "$$\n",
    "\\Vert A\\Vert_{\\infty}=\\max_{1\\le i \\le m}\\sum_{j=1}^n |{a_{ij}}|\n",
    "$$\n",
    "\n",
    "- **矩陣的核範數**：矩陣的奇異值（將矩陣svd分解）之和，這個範數可以用來矩陣的 trace 和來表示（因為最小化核範數，相當於最小化矩陣的秩 - 低秩?），上述矩陣A最終結果就是：10.9287。\n",
    "$$\n",
    "\\Vert A\\Vert=\\sqrt{tr(A^T A)}\n",
    "$$\n",
    "\n",
    "- **矩陣的 L0 範數**：矩陣的非 0 元素的個數，通常用它來表示稀疏，L0 範數越小 0 元素越多，也就越稀疏，上述矩陣 $A$ 最終結果就是：6。\n",
    "\n",
    "- **矩陣的 L1 範數**：矩陣中的每個元素絕對值之和，它是L0範數的最優凸近似，因此它也可以表示稀疏，上述矩陣 $A$ 最終結果就是：22 。\n",
    "\n",
    "- **矩陣的 F 範數**：矩陣的各個元素平方之和再開平方根，它通常也叫做矩陣的L2範數，它的優點在於它是一個凸函數，可以求導求解，易於計算，上述矩陣A最終結果就是：10.0995。\n",
    "  \n",
    "$$\n",
    "\\Vert A\\Vert_F=\\sqrt{(\\sum_{i=1}^m\\sum_{j=1}^n{| a_{ij}|}^2)}\n",
    "$$\n",
    "\n",
    "- **矩陣的L21範數**：矩陣先以每一個 column 為單位，求每一個 row 的 F 範數（也可認為是向量的 2 範數），然後再將得到的結果求 L1 範數（也可認為是向量的 1 範數），很容易看出它是介於 L1 和 L2 之間的一種範數，上述矩陣 $A$ 最終結果就是：17.1559。\n",
    "\n",
    "- **矩陣的 p範數**\n",
    "  \n",
    "$$\n",
    "\\Vert A\\Vert_p=\\sqrt[p]{(\\sum_{i=1}^m\\sum_{j=1}^n{| a_{ij}|}^p)}\n",
    "$$\n",
    "$$\n",
    "\\Vert\\vec{x}\\Vert_{+\\infty}=\\max{|{x_i}|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZUAVLHscTYV"
   },
   "source": [
    "### 1.1.5 如何判斷一個矩陣為正定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pMG5riujcTax"
   },
   "source": [
    "判定一個矩陣是否為正定，通常有以下幾個方面：\n",
    "\n",
    "- 矩陣順序主子式全大於 0\n",
    "- 存在可逆矩陣 $C$ 使 $C^TC$ 等於該矩陣\n",
    "- 正慣性指數等於 $n$\n",
    "- 合同於單位矩陣 $E$（即：規範形為 $E$）\n",
    "- 標準形中主對角元素全為正\n",
    "- 特徵值全為正\n",
    "- 是某基的度量矩陣"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-NT_tlfcTdE"
   },
   "source": [
    "## 1.2 導數和偏導數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvvDpkPlcTfd"
   },
   "source": [
    "### 1.2.1 導數偏導計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JfRQLTE-cThq"
   },
   "source": [
    "**導數定義**:\n",
    "\n",
    "導數(derivative)代表了在自變量變化趨於無窮小的時候，函數值的變化與自變量的變化的比值。幾何意義是這個點的切線。物理意義是該時刻的（瞬時）變化率。\n",
    "​\n",
    "\n",
    "*Note*：在一維函數中，只有一個自變量變動，也就是說只存在一個方向的變化率，這也就是為什麼一維函數沒有偏導數的原因。在物理學中有平均速度和瞬時速度之說。平均速度：\n",
    "\n",
    "$$\n",
    "v=\\frac{s}{t}\n",
    "$$\n",
    "\n",
    "其中 $v$ 表示平均速度，$s$ 表示路程，$t$ 表示時間。這個公式可以改寫為\n",
    "\n",
    "$$\n",
    "\\bar{v}=\\frac{\\Delta s}{\\Delta t}=\\frac{s(t_0+\\Delta t)-s(t_0)}{\\Delta t}\n",
    "$$\n",
    "\n",
    "其中 $\\Delta s$ 表示兩點之間的距離，而 $\\Delta t$ 表示走過這段距離需要花費的時間。當 $\\Delta t$ 趨向於 0（$\\Delta t \\to 0$）時，也就是時間變得很短時，平均速度也就變成了在 $t_0$ 時刻的瞬時速度，表示成如下形式：\n",
    "\n",
    "$$\n",
    "v(t_0)=\\lim_{\\Delta t \\to 0}{\\bar{v}}=\\lim_{\\Delta t \\to 0}{\\frac{\\Delta s}{\\Delta t}}=\\lim_ {\\Delta t \\to 0}{\\frac{s(t_0+\\Delta t)-s(t_0)}{\\Delta t}}\n",
    "$$\n",
    "\n",
    "實際上，上式表示的是路程 $s$ 關於時間 $t$ 的函數在 $t=t_0$ 處的導數。一般的，這樣定義導數：如果平均變化率的極限存在，即有\n",
    "\n",
    "$$\n",
    "\\lim_{\\Delta x \\to 0}{\\frac{\\Delta y}{\\Delta x}}=\\lim_{\\Delta x \\to 0}{\\frac{f(x_0+\\Delta x)-f(x_0 )}{\\Delta x}}\n",
    "$$\n",
    "\n",
    "則稱此極限為函數 $y=f(x)$ 在點 $x_0$ 處的導數。記作 $f'(x_0)$ 或 $y'\\vert_{x=x_0}$ 或 $\\frac{dy}{dx}\\vert_{x=x_0}$ 或 $\\frac{df(x)}{ dx}\\vert_{x=x_0}$。\n",
    "\n",
    "總之，導數就是曲線在某一點切線的斜率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iMEDRUwncTkD"
   },
   "source": [
    "**偏導數**:\n",
    "\n",
    "既然談到偏導數(partial derivative)，那就至少涉及到兩個自變量。以兩個自變量為例，$z=f(x,y)$，從導數到偏導數，也就是從曲線來到了曲面。曲線上的一點，其切線只有一條。但是曲面上的一點，切線有無數條。而偏導數就是指多元函數沿著坐標軸的變化率。\n",
    "\n",
    "\n",
    "*注意*：直觀地說，偏導數也就是函數在某一點上沿坐標軸正方向的的變化率。\n",
    "\n",
    "設函數 $z=f(x,y)$ 在點 $(x_0,y_0)$ 的領域內有定義，當 $y=y_0$ 時，$z$ 可以看作關於 $x$ 的一元函數 $f(x,y_0)$，若該一維函數在 $x=x_0$ 處可導，即有函數的極限$A$存在。那麼稱 $A$ 為函數 $z=f(x,y)$ 在點 $(x_0,y_0)$ 處關於自變量 $x$ 的偏導數，記作 $f_x(x_0,y_0)$ 或 $\\frac{\\partial z}{\\partial x}\\vert_{y=y_0}^{x=x_0}$ 或 $\\frac{\\partial f}{\\partial x}\\vert_{y=y_0}^{x= x_0}$ 或 $z_x\\vert_{y=y_0}^{x=x_0}$。\n",
    "\n",
    "$$\n",
    "A=\\lim_{\\Delta x \\to 0}{\\frac{f(x_0+\\Delta x,y_0)-f(x_0,y_0)}{\\Delta x}}\n",
    "$$\n",
    "\n",
    "\n",
    "偏導數在求解時可以將另外一個變量看做常數，利用普通的求導方式求解，比如 $z=3x^2+xy$ 關於 $x$ 的偏導數就為 $z_x=6x+y$，這個時候 $y$ 相當於 $x$ 的係數。\n",
    "\n",
    "某點 $(x_0,y_0)$ 處的偏導數的幾何意義為曲面 $z=f(x,y)$ 與面 $x=x_0$ 或面 $y=y_0$ 交線在 $y=y_0$ 或 $x=x_0$ 處切線的斜率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uudkQSh3cTmZ"
   },
   "source": [
    "### 1.2.2 導數和偏導數有什麼區別？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IQbAs7ZzcTok"
   },
   "source": [
    "導數和偏導沒有本質區別，如果極限存在，都是當自變量的變化量趨於 0 時，函數值的變化量與自變量變化量比值的極限。\n",
    "\n",
    "> - 一維函數，一個 $y$ 對應一個 $x$，導數只有一個。\n",
    "> - 二維函數，一個 $z$ 對應一個 $x$ 和一個 $y$，有兩個導數：一個是 $z$ 對 $x$ 的導數，一個是 $z$ 對 $y$ 的導數，稱之為偏導。\n",
    "> - 求偏導時要注意，對一個變量求導，則視另一個變量為常數，只對改變量求導，從而將偏導的求解轉化成了一維函數的求導。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EtmooRfXcTrQ"
   },
   "source": [
    "## 1.3 特徵值和特徵向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWDA1rRLcTtz"
   },
   "source": [
    "### 1.3.1 特徵值分解與特徵向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RrMI1wwacTwG"
   },
   "source": [
    "- 特徵值分解可以得到特徵值 (eigenvalues) 與特徵向量 (eigenvectors)\n",
    "- 特徵值表示的是這個特徵到底有多重要，而特徵向量表示這個特徵是什麼。\n",
    "\n",
    "如果說一個向量 $\\vec{v}$ 是方陣 $A$ 的特徵向量，將一定可以表示成下面的形式：\n",
    "$$\n",
    "A\\nu = \\lambda \\nu\n",
    "$$\n",
    "\n",
    "$\\lambda$ 為特徵向量 $\\vec{v}$ 對應的特徵值。特徵值分解是將一個矩陣分解為如下形式：  \n",
    "$$\n",
    "A=Q\\sum Q^{-1}\n",
    "$$\n",
    "\n",
    "其中，$Q$ 是這個矩陣 $A$ 的特徵向量組成的矩陣，$\\sum$ 是一個對角矩陣，每一個對角線元素就是一個特徵值，裡面的特徵值是由大到小排列的，這些特徵值所對應的特徵向量就是描述這個矩陣變化方向（從主要的變化到次要的變化排列）。也就是說矩陣 $A$ 的信息可以由其特徵值和特徵向量表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AeMZELetcTyR"
   },
   "source": [
    "### 1.3.2 奇異值與特徵值有什麼關係"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qxVcnULZcT0y"
   },
   "source": [
    "那麼奇異值和特徵值是怎麼對應起來的呢？我們將一個矩陣$A$的轉置乘以$A$，並對$A^TA$求特徵值，則有下面的形式：\n",
    "\n",
    "$$\n",
    "(A^TA)V = \\lambda V\n",
    "$$\n",
    "\n",
    "這裡 $V$ 就是上面的右奇異向量，另外還有：\n",
    "\n",
    "$$\n",
    "\\sigma_i = \\sqrt{\\lambda_i}, u_i=\\frac{1}{\\sigma_i}A\\mu_i\n",
    "$$\n",
    "\n",
    "這裡的 $\\sigma$ 就是奇異值，$u$就是上面說的左奇異向量。<br>\n",
    "奇異值 $\\sigma$ 跟特徵值類似，在矩陣 $\\sum$ 中也是從大到小排列，而且 $\\sigma$ 的減少特別的快，在很多情況下，前 10% 甚至 1% 的奇異值的和就佔了全部的奇異值之和的 99% 以上了。<br>\n",
    "也就是說，我們也可以用前 $r$（ $r$遠小於$m, n$）個的奇異值來近似描述矩陣，即部分奇異值分解：\n",
    "$$\n",
    "A_{m\\times n}\\approx U_{m \\times r}\\sum_{r\\times r}V_{r \\times n}^T\n",
    "$$\n",
    "\n",
    "右邊的三個矩陣相乘的結果將會是一個接近於 $A$ 的矩陣，在這邊 $r$ 越接近於 $n$，則相乘的結果越接近於 $A$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2nMyhrRvcT28"
   },
   "source": [
    "## 1.4 概率分佈與隨機變量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rvGcFm88cT7j"
   },
   "source": [
    "### 1.4.1 機器學習為什麼要使用概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ax4ZITaicT9r"
   },
   "source": [
    "事件的機率是衡量該事件發生的可能性的量度。雖然在一次隨機試驗中某個事件的發生是帶有偶然性的，但那些可在相同條件下大量重複的隨機試驗卻往往呈現出明顯的數量規律。\n",
    ">- 機器學習除了處理不確定量，也需處理隨機量。不確定性和隨機性可能來自多個方面，使用概率論來量化不確定性。<br>\n",
    ">- 機率論在機器學習中扮演著一個核心角色，因為機器學習算法的設計通常依賴於對數據的概率假設。<br>\n",
    ">- 例如在機器學習（Andrew Ng）的課中，會有一個單純貝氏分類器假設就是條件獨立的一個例子。該學習算法對內容做出假設，用來分辨電子郵件是否為垃圾郵件。假設無論郵件是否為垃圾郵件，單詞 $x$ 出現在郵件中的概率條件獨立於單詞 $y$ 。很明顯這個假設不是 unbiased 的，因為某些單字幾乎總是同時出現。然而，最終結果是，這個簡單的假設對結果的影響並不大，且無論如何都可以讓我們快速判別垃圾郵件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0epudqHZcT_3"
   },
   "source": [
    "### 1.4.2 變量與隨機變量有什麼區別"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j8fFogdBcUCL"
   },
   "source": [
    "**隨機變量**（random variable）\n",
    "\n",
    "表示隨機現象（在一定條件下，並不總是出現相同結果的現象稱為隨機現象）中各種結果的實值函數（一切可能的樣本點）。例如某一時間內公共汽車站等車乘客人數，電話交換台在一定時間內收到的呼叫次數等，都是隨機變量的實例。<br>\n",
    "隨機變量與模糊變量的不確定性的本質差別在於，後者的測定結果仍具有不確定性，即模糊性。\n",
    "\n",
    "**變量與隨機變量的區別：**\n",
    "當變量的取值的概率不是 1 時，變量就變成了隨機變量；當隨機變量取值的概率為 1 時，隨機變量就變成了變量。\n",
    "\n",
    "> Example：\n",
    "> - 當變量 $x$ 值為 100 的機率為 1 的話,那麼 $x=100$ 就是確定條件，不會再有變化，除非有進一步的運算\n",
    "> - 當變量 $x$ 值為 100 的機率為 1，比如為 50 的機率是 0.5，為 100 的機率也還是 0.5，那麼這個變量就是會隨不同條件而變化的，是隨機變量，取到 50 或者 100 的機率都是 0.5，即50%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hCDashfkcUE6"
   },
   "source": [
    "### 1.4.3 隨機變量與機率分佈的關聯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gu5YFQ_kcUHB"
   },
   "source": [
    "一個隨機變量僅表示一個可能取得的狀態，還必須給定與之相伴的機率分佈來製定每個狀態的可能性。<br>\n",
    "用來描述隨機變量或一簇隨機變量的每一個可能的狀態的可能性大小的方法就是 **機率分佈(probability distribution)**.\n",
    "\n",
    "隨機變量可以分為 Discrete Random Variable 和 Continuous Random Variable。\n",
    "\n",
    "相應的描述其機率分佈的函數是\n",
    "\n",
    "概率質量函數(Probability Mass Function, PMF)：描述離散型隨機變量的機率分佈，通常用大寫字母 $P$ 表示。\n",
    "\n",
    "概率密度函數(Probability Density Function, PDF)：描述連續型隨機變量的機率分佈，通常用小寫字母 $p$ 表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CxMXprI5cUJo"
   },
   "source": [
    "### 1.4.4 離散型隨機變量 和 概率質量函數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NiY6cmhXcULy"
   },
   "source": [
    "PMF：將隨機變量所能夠取得的每個狀態投影到隨機變量，並取得該狀態的機率。\n",
    "\n",
    "- 一般而言，$P(x)$ 表示時 $X=x$ 的機率<br>\n",
    "- 有時候為了防止混淆，要明確寫出隨機變量的名稱 $P(X=x)$<br>\n",
    "- 有時候需要先定義一個隨機變量，然後製定它遵循的概率分佈 $x$ 服從 $P(x)$<br>\n",
    "\n",
    "PMF：可以同時作用於多個隨機變量，即聯合機率分佈(joint probability distribution) $P(X=x,Y=y)$ 表示 $X=x$ 和 $Y=y$ 同時發生的概率，也可以簡寫成 $P(x,y)$\n",
    "\n",
    "如果一個函數 $P$ 是隨機變量 $X$ 的 PMF，那麼它必須滿足如下三個條件\n",
    "\n",
    "- $P$ 的定義域必須是的所有可能狀態的集合<br>\n",
    "- $\\forall x \\in x, 0 \\leq P(x) \\leq 1 $\n",
    "- $\\sum_{x \\in X} P(x) = 1$。 我們把這一條性質稱之為：標準化(normalized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FPjvqpR2cUN3"
   },
   "source": [
    "### 1.4.5 連續型隨機變量 和 機率密度函數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gbEBX1RLcUQM"
   },
   "source": [
    "如果一個函數 $p$ 是 x 的 PDF，那麼它必須滿足如下幾個條件\n",
    "\n",
    "- $p$ 的定義域必須是 x 的所有可能狀態的集合。<br>\n",
    "- $\\forall x \\in X, p(x) \\ge 0$<br>\n",
    "Note：我們並不要求 $p(x) \\le 1$，因為此處 $p(x)$ 不是表示的對應此狀態具體的機率，而是機率的一個相對大小(密度)。具體的機率要用積分去求。<br>\n",
    "- $\\int p(x)dx=1$，積分下來，總和還是 1，機率和還是 1\n",
    "\n",
    "Note：PDF $p(x)$ 並沒有直接對特定的狀態給出機率，給出的是密度，相對的，它給出了落在面積為 $\\delta_{x}$ 的無線小區域內機率為 $ p(x) \\delta_{x}$。由此，雖然我們無法求得具體某個狀態的機率，但我們可以得到某個狀態 $x$ 落在某個區間 $[a,b]$ 內的機率為 $ \\int_{a}^{b}p(x)dx$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.6 條件機率舉例說明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "條件機率的公式如下：\n",
    "$$\n",
    "P(A\\mid B) = \\frac{P(A\\cap B)}{P(B)}\n",
    "$$\n",
    "說明：在同一個樣本空間 $\\Omega$ 中的事件或者子集 $A$ 與 $B$，如果隨機從 $\\Omega$ 中選出的一個元素屬於 $B$，那麼下一個隨機選擇的元素屬於$A$ 的機率就定義為在 $B$ 的前提下 $A$ 的條件機率。<br>\n",
    "\n",
    "![條件機率](./img/ch1/conditional_probability.jpg)\n",
    "\n",
    "圖1.1 條件機率文氏圖\n",
    "\n",
    "根據文氏圖，可以很清楚地看到在事件 B 發生的情況下，事件 A 發生的機率就是 $\\frac {P(A\\cap B)}{P(B)}$。<br>\n",
    "\n",
    "Example：一對夫妻有兩個小孩，已知其中一個是女孩，則另一個是女孩子的機率是多少？ （面試、筆試可能會碰到）<br>\n",
    "- **窮舉法**：已知其中一個是女孩，那麼樣本空間為男女，女女，女男，則另外一個仍然是女生的概率就是 $\\frac{1}{3}$<br>\n",
    "- **條件機率法**：$P(Female\\mid Female)=\\frac{P(Female\\cap Female)}{P(女)}$，夫妻有兩個小孩，那麼它的樣本空間為女女，男女，女男，男男，則 $P(Female \\cap Female)$ 為$\\frac{1}{4}$，$P(Female)= 1-P(Male \\cap Male)=\\frac{3}{4}$，所以最後 $\\frac{1}{3}$。<br>\n",
    "\n",
    "這里大家可能會誤會 男女 和 女男 是同一種情況，但實際上姐弟和兄妹便是不同情況。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMLaJ5pUWRna3Z3JBpk6C1Q",
   "collapsed_sections": [],
   "name": "第一章 數學基礎",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
